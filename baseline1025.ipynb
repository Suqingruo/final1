{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d7a957a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib, sys, os, random, time\n",
    "import cv2, gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import torch.cuda.amp as amp  # 导入自动混合精度模块\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR  # 添加余弦退火学习率调度器\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b34afe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE编码和解码函数\n",
    "def rle_encode(im):\n",
    "    '''\n",
    "    im: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = im.flatten(order = 'F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(512, 512)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    if mask_rle == '' or pd.isna(mask_rle):\n",
    "        return np.zeros(shape, dtype=np.uint8)\n",
    "    \n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "774185a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "SEED = 42\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 4\n",
    "IMAGE_SIZE = 320  # 增加图像尺寸以获取更多细节\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "THRESHOLD = 0.5  # 二值化阈值，可以通过验证集调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ae901ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bc96f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增强的数据增强策略\n",
    "# 减少预处理计算负担\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(p=0.5, shift_limit=0.1, scale_limit=0.2, rotate_limit=30, border_mode=0),\n",
    "    A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 验证集变换 - 加在此处\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.Normalize(\n",
    "        mean=[0.625, 0.448, 0.688],\n",
    "        std=[0.131, 0.177, 0.101],\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b3e6f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingSegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_paths, mask_paths=None, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 加载图像\n",
    "        img = cv2.imread(self.img_paths[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 处理掩码数据\n",
    "        if self.mask_paths is not None:\n",
    "            mask_data = self.mask_paths[idx]\n",
    "            \n",
    "            # 使用已有的rle_decode函数处理RLE格式的掩码\n",
    "            if isinstance(mask_data, str):\n",
    "                mask = rle_decode(mask_data, shape=(512, 512))\n",
    "            elif isinstance(mask_data, np.ndarray):\n",
    "                mask = mask_data\n",
    "            elif isinstance(mask_data, torch.Tensor):\n",
    "                mask = mask_data.cpu().numpy()\n",
    "            else:\n",
    "                print(f\"警告：未知的掩码数据类型 - {type(mask_data)}\")\n",
    "                mask = np.zeros((512, 512), dtype=np.uint8)\n",
    "            \n",
    "            # 应用变换\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=img, mask=mask)\n",
    "                img = transformed[\"image\"]\n",
    "                mask = transformed[\"mask\"]\n",
    "            \n",
    "            # 关键修复: 始终确保mask是float类型的tensor\n",
    "            if isinstance(mask, np.ndarray):\n",
    "                mask = torch.from_numpy(mask).float()\n",
    "            else:\n",
    "                # 确保tensor是float类型\n",
    "                mask = mask.float()\n",
    "            \n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0)\n",
    "                \n",
    "            return img, mask\n",
    "        else:\n",
    "            # 只返回图像（用于测试集）\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=img)\n",
    "                img = transformed[\"image\"]\n",
    "                \n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "54a092c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net模型定义 - 基础模块\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv2D -> BN -> ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "733d9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "299417e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        \n",
    "        # 减少通道数\n",
    "        self.inc = DoubleConv(n_channels, 32)\n",
    "        self.down1 = Down(32, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        self.down4 = Down(256, 512 // 2 if bilinear else 512)\n",
    "        self.up1 = Up(512, 256 // 2 if bilinear else 256, bilinear)\n",
    "        self.up2 = Up(256, 128 // 2 if bilinear else 128, bilinear)\n",
    "        self.up3 = Up(128, 64 // 2 if bilinear else 64, bilinear)\n",
    "        self.up4 = Up(64, 32, bilinear)\n",
    "        self.outc = OutConv(32, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        return self.outc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "55372c06-540d-4b2c-aa4c-771e35b2e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedUNet(nn.Module):\n",
    "    def __init__(self, encoder_name=\"efficientnet-b4\", in_channels=3, classes=1):\n",
    "        \"\"\"\n",
    "        初始化具有预训练编码器的高级U-Net模型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 创建模型主干 - 使用预训练的编码器\n",
    "        self.model = smp.UnetPlusPlus(\n",
    "            encoder_name=encoder_name,     # 从 'resnet50', 'efficientnet-b4' 等选择\n",
    "            encoder_weights=\"imagenet\",     # 使用预训练权重\n",
    "            in_channels=in_channels,        # 输入通道数\n",
    "            classes=classes,                # 输出通道数\n",
    "            activation=None,                # 不需要激活，我们会在损失函数中处理\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_model_names():\n",
    "        \"\"\"返回可用的编码器名称列表\"\"\"\n",
    "        return [\n",
    "            \"resnet18\", \"resnet101\",\n",
    "            \"efficientnet-b3\", \"efficientnet-b0\",\n",
    "            \"timm-resnest50d\", \"timm-mobilenetv3_large_100\",\n",
    "            \"densenet121\", \"densenet169\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "085bbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化的损失函数 - Dice Loss\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        \n",
    "        # 平滑处理以避免0/0的情况\n",
    "        intersection = (pred * target).sum(dim=(2,3))\n",
    "        union = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "031b991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = 1e-7  # 添加极小值避免数值不稳定\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # 不再手动应用sigmoid\n",
    "        # 使用binary_cross_entropy_with_logits，它内部会稳定地计算sigmoid+BCE\n",
    "        bce = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
    "        \n",
    "        # 应用focal loss公式\n",
    "        pt = torch.exp(-bce)  # pt = p if y=1, pt = 1-p if y=0\n",
    "        focal_loss = (1-pt)**self.gamma * bce\n",
    "        \n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f0c43a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化的组合损失函数\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, dice_weight=0.5, focal_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.focal_loss = FocalLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        focal = self.focal_loss(pred, target)\n",
    "        return self.dice_weight * dice + self.focal_weight * focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a1422949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"当验证集性能不再提升时提前停止训练\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): 验证集性能不提升后等待多少轮停止训练\n",
    "            verbose (bool): 是否打印详细信息\n",
    "            delta (float): 性能变化的最小阈值\n",
    "            path (str): 保存检查点路径\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'早停计数: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "            \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''当验证损失减小时保存模型'''\n",
    "        if self.verbose:\n",
    "            print(f'验证损失从 ({self.val_loss_min:.6f} 降至 {val_loss:.6f})。保存模型...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9d1a42b1-315a-4034-a96d-6ccba216eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_tensors(images, masks, outputs=None):\n",
    "    \"\"\"检查张量的形状和类型以及值的范围\"\"\"\n",
    "    print(f\"Images: shape={images.shape}, type={images.dtype}, device={images.device}\")\n",
    "    print(f\"Masks: shape={masks.shape}, type={masks.dtype}, device={masks.device}\")\n",
    "    \n",
    "    if outputs is not None:\n",
    "        print(f\"Outputs: shape={outputs.shape}, type={outputs.dtype}, device={outputs.device}\")\n",
    "        \n",
    "        # 检查输出值的范围\n",
    "        with torch.no_grad():\n",
    "            outputs_sigmoid = torch.sigmoid(outputs)\n",
    "            min_val = outputs_sigmoid.min().item()\n",
    "            max_val = outputs_sigmoid.max().item()\n",
    "            print(f\"输出sigmoid后的值范围: [{min_val:.6f}, {max_val:.6f}]\")\n",
    "            \n",
    "            # 检查是否有极端值\n",
    "            if min_val < 0 or max_val > 1:\n",
    "                print(\"警告: sigmoid后的输出值超出[0,1]范围!\")\n",
    "    \n",
    "    # 检查是否包含NaN或Inf\n",
    "    if torch.isnan(images).any():\n",
    "        print(\"警告: 图像包含NaN值!\")\n",
    "    if torch.isnan(masks).any():\n",
    "        print(\"警告: 掩码包含NaN值!\")\n",
    "    if outputs is not None and torch.isnan(outputs).any():\n",
    "        print(\"警告: 输出包含NaN值!\")\n",
    "    if outputs is not None and torch.isinf(outputs).any():\n",
    "        print(\"警告: 输出包含Inf值!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a082a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 创建混合精度训练的梯度缩放器\n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, (images, masks) in enumerate(tqdm(dataloader)):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).float()\n",
    "        \n",
    "        # 仅在特定间隔打印调试信息\n",
    "        if i % 500 == 0:\n",
    "            debug_tensors(images, masks)\n",
    "        \n",
    "        # 使用自动混合精度上下文\n",
    "        with amp.autocast():\n",
    "            # 前向传播\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # 同样，限制输出频率\n",
    "            if i % 500 == 0:\n",
    "                debug_tensors(images, masks, outputs)\n",
    "        \n",
    "        # 在autocast上下文之外计算损失\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        # 检查损失是否有问题\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"警告: 损失值异常: {loss.item()}\")\n",
    "            continue  # 跳过这个批次\n",
    "        \n",
    "        # 使用梯度缩放器进行反向传播    \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # 添加梯度剪裁（在优化器步骤之前）\n",
    "            scaler.unscale_(optimizer)  # 在剪裁前取消梯度缩放\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # 使用梯度缩放器更新权重\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "240d0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"清理可能的内存泄漏\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# 在train_with_checkpoints函数中每个epoch后调用\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3e1f271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证函数\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    dice_scores = []\n",
    "    \n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # 计算Dice分数\n",
    "        preds = (torch.sigmoid(outputs) > THRESHOLD).float()\n",
    "        dice = (2 * (preds * masks).sum()) / (preds.sum() + masks.sum() + 1e-8)\n",
    "        dice_scores.append(dice.item())\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader), np.mean(dice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d321106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "@torch.no_grad()\n",
    "def predict(model, dataloader, device, threshold=THRESHOLD):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for images, filenames in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs)\n",
    "        \n",
    "        # 处理每个批次的预测\n",
    "        for pred, filename in zip(preds, filenames):\n",
    "            pred = pred.cpu().numpy().squeeze()\n",
    "            pred = cv2.resize(pred, (512, 512))  # 调整为原始大小\n",
    "            mask = (pred > threshold).astype(np.uint8)\n",
    "            rle = rle_encode(mask)\n",
    "            results.append([filename, rle])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "344ff872",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_with_tta(model, image, device, threshold=THRESHOLD, tta_transforms=None):\n",
    "    \"\"\"测试时增强提高预测质量\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 如果没有提供TTA变换，则使用基本变换\n",
    "    if tta_transforms is None:\n",
    "        tta_transforms = [\n",
    "            A.Compose([A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]), ToTensorV2()]),\n",
    "            A.Compose([A.HorizontalFlip(p=1.0), A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]), ToTensorV2()]),\n",
    "            A.Compose([A.VerticalFlip(p=1.0), A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]), ToTensorV2()]),\n",
    "            A.Compose([A.Transpose(p=1.0), A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]), ToTensorV2()])\n",
    "        ]\n",
    "    \n",
    "    # 应用所有变换并预测\n",
    "    preds = []\n",
    "    for transform in tta_transforms:\n",
    "        augmented = transform(image=image)\n",
    "        img_tensor = augmented['image'].unsqueeze(0).to(device)\n",
    "        output = model(img_tensor)\n",
    "        pred = torch.sigmoid(output).cpu().numpy().squeeze()\n",
    "        \n",
    "        # 还原变换\n",
    "        if 'HorizontalFlip' in str(transform):\n",
    "            pred = np.fliplr(pred)\n",
    "        if 'VerticalFlip' in str(transform):\n",
    "            pred = np.flipud(pred)\n",
    "        if 'Transpose' in str(transform):\n",
    "            pred = np.transpose(pred)\n",
    "            \n",
    "        preds.append(pred)\n",
    "    \n",
    "    # 平均所有预测结果\n",
    "    final_pred = np.mean(preds, axis=0)\n",
    "    return (final_pred > threshold).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "68d963ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_checkpoints(model, train_loader, valid_loader, optimizer, \n",
    "                          criterion, scheduler, device, num_epochs, \n",
    "                          checkpoint_dir='checkpoints', accumulation_steps=4,\n",
    "                          start_epoch=1, best_dice=0, best_model_epoch=0):  # 添加了这三个参数\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # 如果是继续训练，使用传入的最佳值\n",
    "    best_dice = best_dice\n",
    "    best_epoch = best_model_epoch\n",
    "    \n",
    "    # 创建日志 - 根据是否继续训练决定写入模式\n",
    "    log_mode = \"a\" if start_epoch > 1 else \"w\"\n",
    "    log_file = open(f\"{checkpoint_dir}/training_log.csv\", log_mode)\n",
    "    \n",
    "    # 仅在新训练时写入表头\n",
    "    if start_epoch == 1:\n",
    "        log_file.write(\"epoch,train_loss,val_loss,val_dice,learning_rate\\n\")\n",
    "    \n",
    "    # 初始化早停\n",
    "    early_stopping = EarlyStopping(patience=7, verbose=True, \n",
    "                                   path=f\"{checkpoint_dir}/early_stop_model.pth\")\n",
    "    \n",
    "    # 修改循环范围，从start_epoch开始\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        print(f\"第 {epoch}/{start_epoch + num_epochs - 1} 轮\")\n",
    "        \n",
    "        # 训练 - 使用梯度累积\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, accumulation_steps)\n",
    "        \n",
    "        # 清理内存\n",
    "        clear_memory()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"完成第 {epoch} 轮训练，开始验证...\")\n",
    "\n",
    "        # 验证\n",
    "        val_loss, val_dice = validate(model, valid_loader, criterion, device)\n",
    "\n",
    "        print(f\"验证完成! 损失: {val_loss:.4f}, Dice: {val_dice:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # 清理内存\n",
    "        clear_memory()\n",
    "        \n",
    "        # 记录学习率\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # 写入日志\n",
    "        log_file.write(f\"{epoch},{train_loss:.4f},{val_loss:.4f},{val_dice:.4f},{current_lr:.8f}\\n\")\n",
    "        log_file.flush()\n",
    "        \n",
    "        # 调整学习率\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"训练损失: {train_loss:.4f} | 验证损失: {val_loss:.4f} | Dice分数: {val_dice:.4f} | 学习率: {current_lr:.8f}\")\n",
    "        \n",
    "        # 保存检查点 - 只保存必要信息以节省空间\n",
    "        if epoch % 5 == 0 or epoch == start_epoch + num_epochs - 1:  # 每5个epoch保存一次完整检查点\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_dice': val_dice,\n",
    "                'best_dice': best_dice,\n",
    "                'best_epoch': best_epoch,\n",
    "            }, f\"{checkpoint_dir}/checkpoint_epoch_{epoch}.pth\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_dice > best_dice:\n",
    "            print(f\"Dice分数从 {best_dice:.4f} 提高到 {val_dice:.4f}. 正在保存模型...\")\n",
    "            best_dice = val_dice\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), f\"{checkpoint_dir}/best_model.pth\")\n",
    "        \n",
    "        # 检查早停条件\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"触发早停! 训练停止。\")\n",
    "            break\n",
    "    \n",
    "    log_file.close()\n",
    "    print(f\"训练完成! 最佳Dice分数: {best_dice:.4f} (第{best_epoch}轮)\")\n",
    "    return best_dice, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "74c64172-3e44-4eca-9d7c-42cd114a14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_training(checkpoint_path=None, num_epochs=30):\n",
    "    \"\"\"从检查点恢复训练\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: 检查点路径，可以是以下几种:\n",
    "            - None: 自动寻找model_checkpoints/best_model.pth\n",
    "            - \"best\": 使用model_checkpoints/best_model.pth\n",
    "            - 具体路径: 如\"model_checkpoints/checkpoint_epoch_13.pth\"\n",
    "        num_epochs: 继续训练的轮数\n",
    "    \"\"\"\n",
    "    # 确定要加载的检查点路径\n",
    "    if checkpoint_path is None or checkpoint_path == \"best\":\n",
    "        checkpoint_path = 'model_checkpoints/best_model.pth'\n",
    "    \n",
    "    print(f\"从检查点 {checkpoint_path} 恢复训练...\")\n",
    "    \n",
    "    # 加载数据加载器（与main函数相同）\n",
    "    train_mask = pd.read_csv('数据集/train_mask.csv', sep='\\t', names=['name', 'mask'])\n",
    "    train_mask['name'] = train_mask['name'].apply(lambda x: '数据集/train/' + x)\n",
    "    \n",
    "    # 分割训练集和验证集\n",
    "    train_idx, valid_idx = [], []\n",
    "    for i in range(len(train_mask)):\n",
    "        if i % 7 == 0:\n",
    "            valid_idx.append(i)\n",
    "        else:\n",
    "            train_idx.append(i)\n",
    "    \n",
    "    train_df = train_mask.iloc[train_idx].reset_index(drop=True)\n",
    "    valid_df = train_mask.iloc[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_ds = BuildingSegmentationDataset(\n",
    "        train_df['name'].values,\n",
    "        train_df['mask'].fillna('').values,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    valid_ds = BuildingSegmentationDataset(\n",
    "        valid_df['name'].values,\n",
    "        valid_df['mask'].fillna('').values,\n",
    "        transform=valid_transform\n",
    "    )\n",
    "    \n",
    "    train_loader = D.DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    valid_loader = D.DataLoader(\n",
    "        valid_ds, batch_size=BATCH_SIZE, shuffle=False, \n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 创建模型\n",
    "    model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "    \n",
    "    # 区分是完整检查点还是仅模型权重\n",
    "    if checkpoint_path.endswith('.pth'):\n",
    "        try:\n",
    "            # 尝试作为完整检查点加载\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "                # 这是一个完整检查点\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "                best_dice = checkpoint.get('best_dice', 0)\n",
    "                best_epoch = checkpoint.get('best_epoch', 0)\n",
    "                \n",
    "                # 创建优化器并尝试加载状态\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4, eps=1e-8)\n",
    "                if 'optimizer_state_dict' in checkpoint:\n",
    "                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                    print(\"已加载优化器状态\")\n",
    "                \n",
    "                print(f\"已从完整检查点加载。最佳Dice: {best_dice:.4f} (第{best_epoch}轮)\")\n",
    "                print(f\"继续从第 {start_epoch} 轮开始训练\")\n",
    "            else:\n",
    "                # 这只是模型权重\n",
    "                model.load_state_dict(checkpoint)\n",
    "                start_epoch = 1  # 从第1轮重新开始计数\n",
    "                best_dice = 0\n",
    "                best_epoch = 0\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4, eps=1e-8)\n",
    "                print(\"已加载模型权重，但没有训练状态信息。从第1轮开始训练\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载检查点时出错: {e}\")\n",
    "            print(\"创建新模型...\")\n",
    "            start_epoch = 1\n",
    "            best_dice = 0\n",
    "            best_epoch = 0\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4, eps=1e-8)\n",
    "    else:\n",
    "        raise ValueError(f\"不支持的检查点路径: {checkpoint_path}\")\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # 创建学习率调度器\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=num_epochs,\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # 损失函数\n",
    "    criterion = CombinedLoss(dice_weight=0.8, focal_weight=0.2)\n",
    "    \n",
    "    # 继续训练\n",
    "    best_dice, best_epoch = train_with_checkpoints(\n",
    "        model, train_loader, valid_loader, optimizer,\n",
    "        criterion, scheduler, DEVICE, num_epochs,\n",
    "        checkpoint_dir='model_checkpoints',\n",
    "        start_epoch=start_epoch,\n",
    "        best_dice=best_dice,\n",
    "        best_model_epoch=best_epoch\n",
    "    )\n",
    "    \n",
    "    return model, best_dice, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "78b55041-7794-4253-9863-afddecfceaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiple_models(encoders=[\"efficientnet-b0\", \"resnet18\"], \n",
    "                         epochs=EPOCHS,  # 默认使用全局EPOCHS\n",
    "                         save_dir=\"model_ensemble\",\n",
    "                         include_existing_model=True,\n",
    "                         existing_model_path=\"model_checkpoints/best_model.pth\",\n",
    "                         batch_size=BATCH_SIZE,  # 默认使用全局BATCH_SIZE\n",
    "                         image_size=IMAGE_SIZE,  # 默认使用全局IMAGE_SIZE\n",
    "                         accumulation_steps=16):\n",
    "    \"\"\"训练多个不同编码器的模型用于集成，优化内存使用\"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    trained_models = []\n",
    "    best_scores = []\n",
    "    encoders_used = []\n",
    "    \n",
    "    # 如果包含现有模型，将其添加到列表中\n",
    "    if include_existing_model and os.path.exists(existing_model_path):\n",
    "        print(f\"包含现有模型: {existing_model_path}\")\n",
    "        trained_models.append(existing_model_path)\n",
    "        best_scores.append(0.86)\n",
    "        encoders_used.append(\"unet\")\n",
    "    \n",
    "    # 数据加载和转换 - 使用新的image_size\n",
    "    train_transform_local = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(p=0.5, shift_limit=0.1, scale_limit=0.2, rotate_limit=30, border_mode=0),\n",
    "        A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    valid_transform_local = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    # 加载数据\n",
    "    try:\n",
    "        print(\"正在加载训练数据...\")\n",
    "        train_mask = pd.read_csv('数据集/train_mask.csv', sep='\\t', names=['name', 'mask'])\n",
    "        train_mask['name'] = train_mask['name'].apply(lambda x: '数据集/train/' + x)\n",
    "    except Exception as e:\n",
    "        print(f\"加载数据时出错: {e}\")\n",
    "        return [], [], []\n",
    "    \n",
    "    # 分割训练集和验证集\n",
    "    train_idx, valid_idx = [], []\n",
    "    for i in range(len(train_mask)):\n",
    "        if i % 7 == 0:\n",
    "            valid_idx.append(i)\n",
    "        else:\n",
    "            train_idx.append(i)\n",
    "    \n",
    "    train_df = train_mask.iloc[train_idx].reset_index(drop=True)\n",
    "    valid_df = train_mask.iloc[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"训练集: {len(train_df)} 样本, 验证集: {len(valid_df)} 样本\")\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_ds = BuildingSegmentationDataset(\n",
    "        train_df['name'].values,\n",
    "        train_df['mask'].fillna('').values,\n",
    "        transform=train_transform_local\n",
    "    )\n",
    "    \n",
    "    valid_ds = BuildingSegmentationDataset(\n",
    "        valid_df['name'].values,\n",
    "        valid_df['mask'].fillna('').values,\n",
    "        transform=valid_transform_local\n",
    "    )\n",
    "    \n",
    "    # 使用传入的batch_size\n",
    "    train_loader = D.DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True, \n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    valid_loader = D.DataLoader(\n",
    "        valid_ds, batch_size=batch_size, shuffle=False, \n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 训练每个模型\n",
    "    for i, encoder_name in enumerate(encoders):\n",
    "        print(f\"\\n训练模型 {i+1}/{len(encoders)}: 使用编码器 {encoder_name}\")\n",
    "        \n",
    "        # 在每个新模型训练前，清理内存\n",
    "        clear_memory()\n",
    "        \n",
    "        try:\n",
    "            # 创建优化后的模型\n",
    "            model = smp.UnetPlusPlus(\n",
    "                encoder_name=encoder_name,\n",
    "                encoder_weights=\"imagenet\",\n",
    "                in_channels=3,\n",
    "                classes=1,\n",
    "                activation=None,\n",
    "                decoder_channels=(256, 128, 64, 32, 16)  # 更小的解码器\n",
    "            )\n",
    "            model.to(DEVICE)\n",
    "            \n",
    "            # 打印模型信息\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"模型总参数量: {total_params:,}\")\n",
    "            \n",
    "            # 优化器和调度器\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4, eps=1e-8)\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "            \n",
    "            # 损失函数\n",
    "            criterion = CombinedLoss(dice_weight=0.8, focal_weight=0.2)\n",
    "            \n",
    "            # 训练\n",
    "            model_save_dir = f\"{save_dir}/model_{i+1}_{encoder_name}\"\n",
    "            os.makedirs(model_save_dir, exist_ok=True)\n",
    "            \n",
    "            best_dice, best_epoch = train_with_checkpoints(\n",
    "                model, train_loader, valid_loader, optimizer,\n",
    "                criterion, scheduler, DEVICE, epochs,\n",
    "                checkpoint_dir=model_save_dir,\n",
    "                accumulation_steps=accumulation_steps\n",
    "            )\n",
    "            \n",
    "            print(f\"模型 {i+1} 训练完成! 最佳Dice: {best_dice:.4f} (第{best_epoch}轮)\")\n",
    "            trained_models.append(f\"{model_save_dir}/best_model.pth\")\n",
    "            best_scores.append(best_dice)\n",
    "            encoders_used.append(encoder_name)\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e).lower():\n",
    "                print(f\"在训练 {encoder_name} 时内存不足。尝试更小的编码器或增大累积步数。\")\n",
    "                clear_memory()\n",
    "            else:\n",
    "                print(f\"训练 {encoder_name} 时出错: {e}\")\n",
    "        \n",
    "        # 清理内存\n",
    "        clear_memory()\n",
    "    \n",
    "    # 返回训练好的模型信息\n",
    "    result_df = pd.DataFrame({\n",
    "        'model_path': trained_models,\n",
    "        'encoder': encoders_used,\n",
    "        'best_dice': best_scores\n",
    "    })\n",
    "    result_df.to_csv(f\"{save_dir}/ensemble_models_info.csv\", index=False)\n",
    "    \n",
    "    return trained_models, encoders_used, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "aee8ce79-ce5c-4c87-80af-af6b12844c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_state_dict_keys(state_dict, model_type):\n",
    "    \"\"\"修复状态字典的键名以匹配模型架构\n",
    "    \n",
    "    Args:\n",
    "        state_dict: 需要修复的状态字典\n",
    "        model_type: 模型类型，\"unet\"或其他（如\"efficientnet-b0\"）\n",
    "    \n",
    "    Returns:\n",
    "        修复后的状态字典\n",
    "    \"\"\"\n",
    "    new_state_dict = {}\n",
    "    \n",
    "    if model_type == \"unet\":\n",
    "        # 原始UNet不需要修改\n",
    "        return state_dict\n",
    "    else:\n",
    "        # 为高级模型修复键名\n",
    "        for key, value in state_dict.items():\n",
    "            if key.startswith(\"encoder\") or key.startswith(\"decoder\") or key.startswith(\"segmentation_head\"):\n",
    "                # 添加\"model.\"前缀\n",
    "                new_key = f\"model.{key}\"\n",
    "                new_state_dict[new_key] = value\n",
    "            else:\n",
    "                new_state_dict[key] = value\n",
    "    \n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6f2d3446-8f24-4ae8-9f86-c61bd606ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ensemble_models(model_paths, encoder_names):\n",
    "    \"\"\"加载多个模型用于集成，支持修复键名不匹配问题\"\"\"\n",
    "    \n",
    "    assert len(model_paths) == len(encoder_names), \"模型路径和编码器名称数量必须相同\"\n",
    "    \n",
    "    models = []\n",
    "    for path, encoder in zip(model_paths, encoder_names):\n",
    "        if encoder == \"unet\":\n",
    "            # 原始UNet模型\n",
    "            model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "        else:\n",
    "            # 高级模型\n",
    "            model = AdvancedUNet(encoder_name=encoder)\n",
    "        \n",
    "        # 加载权重并修复键名\n",
    "        state_dict = torch.load(path)\n",
    "        fixed_state_dict = fix_state_dict_keys(state_dict, encoder)\n",
    "        \n",
    "        # 加载修复后的状态字典\n",
    "        try:\n",
    "            model.load_state_dict(fixed_state_dict)\n",
    "            print(f\"成功加载模型: {path} (编码器: {encoder})\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载模型失败: {path} (编码器: {encoder})\")\n",
    "            print(f\"错误: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "04a93714-da0b-4d6f-bf37-613f55d4d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_ensemble(models, image_path, device=DEVICE, threshold=0.5):\n",
    "    \"\"\"使用模型集成进行预测\"\"\"\n",
    "    # 读取图像\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 应用测试时增强(TTA)\n",
    "    tta_transforms = [\n",
    "        A.Compose([\n",
    "            A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        A.Compose([\n",
    "            A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            A.HorizontalFlip(p=1.0),\n",
    "            A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        A.Compose([\n",
    "            A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            A.VerticalFlip(p=1.0),\n",
    "            A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    # 所有模型和所有TTA的预测结果\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 对每个模型进行预测\n",
    "        for model in models:\n",
    "            model_preds = []\n",
    "            \n",
    "            # 对每个TTA变换进行预测\n",
    "            for transform in tta_transforms:\n",
    "                # 应用变换\n",
    "                augmented = transform(image=image)\n",
    "                img_tensor = augmented['image'].unsqueeze(0).to(device)\n",
    "                \n",
    "                # 预测\n",
    "                output = model(img_tensor)\n",
    "                pred = torch.sigmoid(output).cpu().numpy().squeeze()\n",
    "                \n",
    "                # 还原变换\n",
    "                if 'HorizontalFlip' in str(transform):\n",
    "                    pred = np.fliplr(pred)\n",
    "                if 'VerticalFlip' in str(transform):\n",
    "                    pred = np.flipud(pred)\n",
    "                \n",
    "                # 调整大小到原始尺寸\n",
    "                pred = cv2.resize(pred, (512, 512))\n",
    "                model_preds.append(pred)\n",
    "            \n",
    "            # 平均单个模型的所有TTA预测\n",
    "            avg_pred = np.mean(model_preds, axis=0)\n",
    "            all_preds.append(avg_pred)\n",
    "    \n",
    "    # 平均所有模型的预测\n",
    "    final_pred = np.mean(all_preds, axis=0)\n",
    "    \n",
    "    # 应用阈值得到二值分割图\n",
    "    binary_mask = (final_pred > threshold).astype(np.uint8)\n",
    "    \n",
    "    return binary_mask, final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "69e4e0f1-0293-4af0-a7d9-df63a5b62059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble_on_valid(model_paths, encoder_names, threshold=0.5):\n",
    "    \"\"\"在验证集上评估单个模型和集成的表现\"\"\"\n",
    "    # 加载模型\n",
    "    models = load_ensemble_models(model_paths, encoder_names)\n",
    "    \n",
    "    # 加载验证数据\n",
    "    train_mask = pd.read_csv('数据集/train_mask.csv', sep='\\t', names=['name', 'mask'])\n",
    "    train_mask['name'] = train_mask['name'].apply(lambda x: '数据集/train/' + x)\n",
    "    \n",
    "    # 分割训练集和验证集\n",
    "    valid_idx = []\n",
    "    for i in range(len(train_mask)):\n",
    "        if i % 7 == 0:\n",
    "            valid_idx.append(i)\n",
    "    \n",
    "    valid_df = train_mask.iloc[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    # 评估每个单个模型\n",
    "    individual_scores = []\n",
    "    for i, model in enumerate(models):\n",
    "        dice_scores = []\n",
    "        model.eval()\n",
    "        \n",
    "        for idx, row in tqdm(valid_df.iterrows(), desc=f\"评估模型 {i+1}/{len(models)}\"):\n",
    "            image_path = row['name']\n",
    "            \n",
    "            # 读取图像和真实掩码\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            mask_gt = rle_decode(row['mask'], shape=(512, 512))\n",
    "            \n",
    "            # 预测掩码\n",
    "            with torch.no_grad():\n",
    "                transformed = valid_transform(image=image)\n",
    "                img_tensor = transformed['image'].unsqueeze(0).to(DEVICE)\n",
    "                output = model(img_tensor)\n",
    "                pred = torch.sigmoid(output).cpu().numpy().squeeze()\n",
    "                pred = cv2.resize(pred, (512, 512))\n",
    "                pred_mask = (pred > threshold).astype(np.uint8)\n",
    "            \n",
    "            # 计算Dice分数\n",
    "            dice = (2.0 * (pred_mask * mask_gt).sum()) / (pred_mask.sum() + mask_gt.sum() + 1e-8)\n",
    "            dice_scores.append(dice)\n",
    "            \n",
    "            # 每10个样本清理一次内存\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                clear_memory()\n",
    "        \n",
    "        # 记录这个模型的分数\n",
    "        model_score = np.mean(dice_scores)\n",
    "        individual_scores.append(model_score)\n",
    "        print(f\"模型 {i+1} 平均Dice分数: {model_score:.4f}\")\n",
    "    \n",
    "    # 评估集成模型\n",
    "    ensemble_dice_scores = []\n",
    "    for idx, row in tqdm(valid_df.iterrows(), desc=\"评估模型集成\"):\n",
    "        image_path = row['name']\n",
    "        mask_gt = rle_decode(row['mask'], shape=(512, 512))\n",
    "        \n",
    "        try:\n",
    "            # 使用集成预测\n",
    "            pred_mask, _ = predict_with_ensemble(models, image_path, DEVICE, threshold)\n",
    "            \n",
    "            # 计算Dice分数\n",
    "            dice = (2.0 * (pred_mask * mask_gt).sum()) / (pred_mask.sum() + mask_gt.sum() + 1e-8)\n",
    "            ensemble_dice_scores.append(dice)\n",
    "            \n",
    "            # 每10个样本清理一次内存\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                clear_memory()\n",
    "        except Exception as e:\n",
    "            print(f\"处理图像 {image_path} 时出错: {str(e)}\")\n",
    "    \n",
    "    # 计算集成的平均分数\n",
    "    ensemble_score = np.mean(ensemble_dice_scores)\n",
    "    print(f\"\\n集成模型的平均Dice分数: {ensemble_score:.4f}\")\n",
    "    \n",
    "    # 打印比较\n",
    "    print(\"\\n性能比较:\")\n",
    "    for i, score in enumerate(individual_scores):\n",
    "        print(f\"模型 {i+1}: {score:.4f}\")\n",
    "    print(f\"集成: {ensemble_score:.4f}\")\n",
    "    \n",
    "    # 计算集成提升\n",
    "    best_individual = max(individual_scores)\n",
    "    improvement = ensemble_score - best_individual\n",
    "    print(f\"\\n集成相比最好单模型提升: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "    \n",
    "    return individual_scores, ensemble_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a4b5fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 设置随机种子\n",
    "    seed_everything(SEED)\n",
    "    \n",
    "    # 加载数据\n",
    "    try:\n",
    "        print(\"正在加载训练数据...\")\n",
    "        train_mask = pd.read_csv('数据集/train_mask.csv', sep='\\t', names=['name', 'mask'])\n",
    "        train_mask['name'] = train_mask['name'].apply(lambda x: '数据集/train/' + x)\n",
    "    except Exception as e:\n",
    "        print(f\"加载数据时出错: {e}\")\n",
    "        print(\"请确保'数据集/train_mask.csv'文件存在并且格式正确!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"已加载 {len(train_mask)} 条训练数据\")\n",
    "    \n",
    "    # 分割训练集和验证集\n",
    "    train_idx, valid_idx = [], []\n",
    "    for i in range(len(train_mask)):\n",
    "        if i % 7 == 0:\n",
    "            valid_idx.append(i)\n",
    "        else:\n",
    "            train_idx.append(i)\n",
    "    \n",
    "    train_df = train_mask.iloc[train_idx].reset_index(drop=True)\n",
    "    valid_df = train_mask.iloc[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"训练集: {len(train_df)} 样本, 验证集: {len(valid_df)} 样本\")\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    train_ds = BuildingSegmentationDataset(\n",
    "        train_df['name'].values,\n",
    "        train_df['mask'].fillna('').values,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    valid_ds = BuildingSegmentationDataset(\n",
    "        valid_df['name'].values,\n",
    "        valid_df['mask'].fillna('').values,\n",
    "        transform=valid_transform\n",
    "    )\n",
    "    \n",
    "    train_loader = D.DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    valid_loader = D.DataLoader(\n",
    "        valid_ds, batch_size=BATCH_SIZE, shuffle=False, \n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # 打印模型摘要\n",
    "    print(f\"模型已创建并加载到设备: {DEVICE}\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"模型总参数量: {total_params:,}\")\n",
    "    \n",
    "    # 使用余弦退火学习率调度\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4, eps=1e-8)\n",
    "    \n",
    "    # 替换为余弦退火学习率调度器\n",
    "    # T_max为总周期数，即训练的总轮次\n",
    "    # eta_min为最小学习率\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=EPOCHS,  # 一个完整的余弦周期\n",
    "        eta_min=1e-6   # 最小学习率\n",
    "    )\n",
    "    \n",
    "    # 损失函数\n",
    "    criterion = CombinedLoss(dice_weight=0.8, focal_weight=0.2)  # 增加Dice权重\n",
    "    \n",
    "    # 使用改进的训练循环\n",
    "    best_dice, best_epoch = train_with_checkpoints(\n",
    "        model, train_loader, valid_loader, optimizer, \n",
    "        criterion, scheduler, DEVICE, EPOCHS, checkpoint_dir='model_checkpoints'\n",
    "    )\n",
    "    \n",
    "    print(f\"训练完成! 最佳Dice分数: {best_dice:.4f} 在第{best_epoch}轮\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7673b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_set():\n",
    "    # 加载最佳模型进行预测\n",
    "    model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('model_checkpoints/best_model.pth'))\n",
    "    except:\n",
    "        model.load_state_dict(torch.load('best_building_segmentation_model.pth'))\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # 创建测试数据集\n",
    "    test_paths = []\n",
    "    # 确保这个路径指向您的测试图像目录\n",
    "    for file in os.listdir('数据集/test_a'):\n",
    "        if file.endswith('.jpg') or file.endswith('.tif'):\n",
    "            test_paths.append(os.path.join('数据集/test_a', file))\n",
    "    \n",
    "    # 使用更小的批次\n",
    "    test_batch_size = 1  # 单张预测避免内存问题\n",
    "    \n",
    "    test_ds = BuildingSegmentationDataset(\n",
    "        test_paths,\n",
    "        None,  # 测试集没有掩码数据\n",
    "        transform=valid_transform\n",
    "    )\n",
    "    \n",
    "    test_loader = D.DataLoader(\n",
    "        test_ds, batch_size=test_batch_size, shuffle=False,\n",
    "        num_workers=1, pin_memory=True  # 使用更少的worker和更小的批量\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    test_files = [os.path.basename(p) for p in test_paths]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for images in tqdm(test_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            \n",
    "            # 处理每个批次的预测\n",
    "            for pred in preds:\n",
    "                if i >= len(test_files):  # 安全检查\n",
    "                    break\n",
    "                    \n",
    "                pred = pred.cpu().numpy().squeeze()\n",
    "                # 及时清理GPU内存\n",
    "                clear_memory()\n",
    "                \n",
    "                pred = cv2.resize(pred, (512, 512))  # 调整为原始大小\n",
    "                mask = (pred > THRESHOLD).astype(np.uint8)\n",
    "                rle = rle_encode(mask)\n",
    "                results.append([test_files[i], rle])\n",
    "                i += 1\n",
    "    \n",
    "    submission = pd.DataFrame(results, columns=['name', 'mask'])\n",
    "    submission.to_csv('submission.csv', index=False, header=False, sep='\\t')\n",
    "    print(\"预测完成! 结果已保存到 submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4e7c6837-a18c-4046-9336-46bd2e9c65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_with_ensemble(model_paths=None, encoder_names=None, threshold=0.5):\n",
    "    \"\"\"使用模型集成预测测试集，确保符合比赛提交格式要求\"\"\"\n",
    "    # 如果没有提供模型路径，使用默认值\n",
    "    if model_paths is None:\n",
    "        model_paths = [\n",
    "            \"model_checkpoints/best_model.pth\",\n",
    "            \"model_ensemble/model_1_efficientnet-b0/best_model.pth\",\n",
    "            \"model_ensemble/model_1_resnet18/best_model.pth\"\n",
    "        ]\n",
    "    \n",
    "    # 如果没有提供编码器名称，使用默认值\n",
    "    if encoder_names is None:\n",
    "        encoder_names = [\"unet\", \"efficientnet-b0\", \"resnet18\"]\n",
    "    \n",
    "    # 加载模型\n",
    "    models = load_ensemble_models(model_paths, encoder_names)\n",
    "    if not models:\n",
    "        print(\"没有可用的模型，无法进行预测。\")\n",
    "        return\n",
    "    \n",
    "    print(f\"成功加载 {len(models)} 个模型进行集成预测\")\n",
    "    \n",
    "    # 读取样本提交文件，确保顺序和格式一致\n",
    "    try:\n",
    "        # 使用header=None参数，防止第一行被当作列名\n",
    "        sample_submit = pd.read_csv('数据集/test_a_samplesubmit.csv', header=None)\n",
    "        print(f\"读取样本提交文件成功，包含 {len(sample_submit)} 个测试样本\")\n",
    "        \n",
    "        # 手动添加列名\n",
    "        if len(sample_submit.columns) == 2:\n",
    "            sample_submit.columns = ['name', 'mask']\n",
    "        else:\n",
    "            sample_submit.columns = ['name']\n",
    "        \n",
    "        # 清理文件名中的额外空格\n",
    "        sample_submit['name'] = sample_submit['name'].str.strip()\n",
    "        \n",
    "        print(f\"样本文件有 {len(sample_submit)} 行数据\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取样本提交文件失败: {e}\")\n",
    "        print(\"尝试直接从测试文件夹读取图像...\")\n",
    "        test_dir = '数据集/test_a'\n",
    "        sample_submit = pd.DataFrame({\n",
    "            'name': [f for f in os.listdir(test_dir) if f.endswith('.jpg') or f.endswith('.tif')]\n",
    "        })\n",
    "    \n",
    "    # 创建结果DataFrame\n",
    "    results = pd.DataFrame({'name': sample_submit['name'].values})\n",
    "    results['mask'] = ''  # 初始化为空字符串\n",
    "    \n",
    "    # 记录测试图像的完整路径\n",
    "    test_dir = '数据集/test_a'\n",
    "    test_paths = {}\n",
    "    for f in os.listdir(test_dir):\n",
    "        if f.endswith('.jpg') or f.endswith('.tif'):\n",
    "            # 确保文件名的键也被清理\n",
    "            test_paths[f.strip()] = os.path.join(test_dir, f)\n",
    "    \n",
    "    # 检查文件名匹配情况\n",
    "    missing_files = []\n",
    "    for filename in results['name']:\n",
    "        if filename not in test_paths:\n",
    "            missing_files.append(filename)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"警告: 有 {len(missing_files)} 个文件名在测试目录中找不到\")\n",
    "        print(\"前5个缺失的文件名:\", missing_files[:5])\n",
    "        print(\"测试目录中的第一个文件名:\", list(test_paths.keys())[0])\n",
    "        \n",
    "        # 尝试进行更细致的匹配\n",
    "        test_files_set = set(test_paths.keys())\n",
    "        for missing in missing_files[:]:  # 创建副本进行遍历\n",
    "            # 检查去除扩展名后是否匹配\n",
    "            missing_base = os.path.splitext(missing)[0]\n",
    "            for test_file in test_files_set:\n",
    "                test_base = os.path.splitext(test_file)[0]\n",
    "                if missing_base == test_base:\n",
    "                    print(f\"找到替代匹配: '{missing}' -> '{test_file}'\")\n",
    "                    # 更新test_paths字典以包含两种形式\n",
    "                    test_paths[missing] = test_paths[test_file]\n",
    "                    missing_files.remove(missing)\n",
    "                    break\n",
    "    \n",
    "    # 对每个测试图像进行预测\n",
    "    for idx, row in tqdm(results.iterrows(), desc=\"预测测试集\"):\n",
    "        filename = row['name']\n",
    "        if filename not in test_paths:\n",
    "            print(f\"警告: 测试图像 '{filename}' 不存在，将使用空RLE\")\n",
    "            continue\n",
    "            \n",
    "        image_path = test_paths[filename]\n",
    "        \n",
    "        try:\n",
    "            # 读取图像\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # 使用集成模型预测\n",
    "            all_preds = []\n",
    "            with torch.no_grad():  # 添加这一行\n",
    "                for model in models:\n",
    "                    # 进行测试时增强预测\n",
    "                    model_preds = []\n",
    "                    \n",
    "                    # 基本变换\n",
    "                    augmented = valid_transform(image=image)\n",
    "                    img_tensor = augmented['image'].unsqueeze(0).to(DEVICE)\n",
    "                    output = model(img_tensor)\n",
    "                    pred = torch.sigmoid(output).detach().cpu().numpy().squeeze()  # 添加.detach()\n",
    "                    pred = cv2.resize(pred, (512, 512))\n",
    "                    model_preds.append(pred)\n",
    "                    \n",
    "                    # 水平翻转\n",
    "                    flip_h_transform = A.Compose([\n",
    "                        A.HorizontalFlip(p=1.0),\n",
    "                        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                        A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]),\n",
    "                        ToTensorV2()\n",
    "                    ])\n",
    "                    augmented = flip_h_transform(image=image)\n",
    "                    img_tensor = augmented['image'].unsqueeze(0).to(DEVICE)\n",
    "                    output = model(img_tensor)\n",
    "                    pred = torch.sigmoid(output).detach().cpu().numpy().squeeze()  # 添加.detach()\n",
    "                    pred = cv2.resize(pred, (512, 512))\n",
    "                    pred = np.fliplr(pred)\n",
    "                    model_preds.append(pred)\n",
    "                    \n",
    "                    # 垂直翻转\n",
    "                    flip_v_transform = A.Compose([\n",
    "                        A.VerticalFlip(p=1.0),\n",
    "                        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                        A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]),\n",
    "                        ToTensorV2()\n",
    "                    ])\n",
    "                    augmented = flip_v_transform(image=image)\n",
    "                    img_tensor = augmented['image'].unsqueeze(0).to(DEVICE)\n",
    "                    output = model(img_tensor)\n",
    "                    pred = torch.sigmoid(output).detach().cpu().numpy().squeeze()  # 添加.detach()\n",
    "                    pred = cv2.resize(pred, (512, 512))\n",
    "                    pred = np.flipud(pred)\n",
    "                    model_preds.append(pred)\n",
    "                    \n",
    "                    # 平均单个模型的各种预测\n",
    "                    avg_pred = np.mean(model_preds, axis=0)\n",
    "                    all_preds.append(avg_pred)\n",
    "            \n",
    "            # 平均所有模型的预测\n",
    "            final_pred = np.mean(all_preds, axis=0)\n",
    "            mask = (final_pred > threshold).astype(np.uint8)\n",
    "            rle = rle_encode(mask)\n",
    "            \n",
    "            # 保存到结果DataFrame对应位置\n",
    "            results.loc[idx, 'mask'] = rle\n",
    "            \n",
    "            # 每50个样本清理一次内存\n",
    "            if (idx + 1) % 50 == 0:\n",
    "                clear_memory()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"处理图像 {filename} 时出错: {str(e)}\")\n",
    "            traceback.print_exc()  # 添加这行以获取更详细的错误信息\n",
    "            # 错误时保留空字符串\n",
    "    \n",
    "    # 检查提交文件中是否有原始空格，确保保持与样本文件一致的格式\n",
    "    original_sample = pd.read_csv('数据集/test_a_samplesubmit.csv', header=None)\n",
    "    first_line = original_sample.iloc[0, 0]\n",
    "    print(f\"原始样本第一行: '{first_line}'\")\n",
    "    \n",
    "    has_trailing_space = False\n",
    "    if first_line.endswith(' '):\n",
    "        print(\"检测到原始文件名后有空格，保持此格式...\")\n",
    "        has_trailing_space = True\n",
    "        \n",
    "    # 如果原始文件有空格，添加回去\n",
    "    if has_trailing_space:\n",
    "        results['name'] = results['name'].apply(lambda x: x + ' ')\n",
    "    \n",
    "    # 创建符合要求的提交文件\n",
    "    # 使用逗号作为分隔符，不保留索引，不写入列名\n",
    "    # 注意: 如果是空格分隔，需要修改sep参数\n",
    "    sep_char = ' ' if first_line.endswith(' ') else ','\n",
    "    results.to_csv('submission_ensemble.csv', index=False, header=False, sep=sep_char)\n",
    "    print(f\"预测完成! 结果保存为 submission_ensemble.csv，共 {len(results)} 个样本\")\n",
    "    print(f\"使用分隔符: '{sep_char}'\")\n",
    "    \n",
    "    # 检查生成的提交文件\n",
    "    try:\n",
    "        # 手动读取第一行以检查格式\n",
    "        with open('submission_ensemble.csv', 'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "        print(f\"生成文件的第一行: '{first_line}'\")\n",
    "        \n",
    "        # 再次检查完整文件\n",
    "        check_df = pd.read_csv('submission_ensemble.csv', header=None, sep=sep_char)\n",
    "        print(f\"生成的提交文件包含 {len(check_df)} 行\")\n",
    "        \n",
    "        if len(check_df) != len(sample_submit):\n",
    "            print(f\"警告: 生成的文件行数 ({len(check_df)}) 与样本提交文件行数 ({len(sample_submit)}) 不一致!\")\n",
    "        else:\n",
    "            print(\"行数检查通过!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"提交文件检查失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "05502a4f-25cc-4e40-92f0-4827677a6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_ensemble_pipeline(train_new=True, include_existing=True):\n",
    "    \"\"\"执行完整的集成模型训练和预测流程，可选包含现有模型\"\"\"\n",
    "    \n",
    "    # 设置随机种子\n",
    "    seed_everything(SEED)\n",
    "    \n",
    "    # 模型编码器列表 - 只需要训练2个新模型，现有模型为第3个\n",
    "    encoders = [\"efficientnet-b0\", \"resnet18\"]\n",
    "    \n",
    "    \n",
    "    if train_new:\n",
    "        print(\"开始训练模型用于集成...\")\n",
    "        model_paths, encoder_names, best_scores = train_multiple_models(\n",
    "            encoders=encoders,\n",
    "            save_dir=\"model_ensemble\",\n",
    "            include_existing_model=include_existing,\n",
    "            existing_model_path=\"model_checkpoints/best_model.pth\"  # 您现有模型的路径\n",
    "        )\n",
    "    else:\n",
    "        # 如果已经训练好了，直接加载路径\n",
    "        try:\n",
    "            ensemble_info = pd.read_csv(\"model_ensemble/ensemble_models_info.csv\")\n",
    "            model_paths = ensemble_info['model_path'].tolist()\n",
    "            encoder_names = ensemble_info['encoder'].tolist()\n",
    "        except:\n",
    "            # 如果CSV不存在，使用默认路径\n",
    "            if include_existing:\n",
    "                model_paths = [\n",
    "                    \"model_checkpoints/best_model.pth\",  # 现有模型\n",
    "                    \"model_ensemble/model_1_efficientnet-b4/best_model.pth\",\n",
    "                    \"model_ensemble/model_2_resnet50/best_model.pth\"\n",
    "                ]\n",
    "                encoder_names = [\"unet\", \"efficientnet-b4\", \"resnet50\"]\n",
    "            else:\n",
    "                model_paths = [\n",
    "                    \"model_ensemble/model_1_efficientnet-b4/best_model.pth\",\n",
    "                    \"model_ensemble/model_2_resnet50/best_model.pth\"\n",
    "                ]\n",
    "                encoder_names = [\"efficientnet-b4\", \"resnet50\"]\n",
    "    \n",
    "    # 在验证集上评估集成性能\n",
    "    _, ensemble_score = evaluate_ensemble_on_valid(\n",
    "        model_paths, \n",
    "        encoder_names=encoder_names\n",
    "    )\n",
    "    \n",
    "    # 生成测试集预测\n",
    "    predict_test_with_ensemble(model_paths, encoder_names=encoder_names)\n",
    "    \n",
    "    return ensemble_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ca55377a-321b-4b83-955f-2f7d874c17f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载模型: model_checkpoints/best_model.pth (编码器: unet)\n",
      "成功加载模型: model_ensemble/model_1_efficientnet-b0/best_model.pth (编码器: efficientnet-b0)\n",
      "成功加载模型: model_ensemble/model_1_resnet18/best_model.pth (编码器: resnet18)\n",
      "成功加载 3 个模型进行集成预测\n",
      "读取样本提交文件成功，包含 2500 个测试样本\n",
      "样本文件有 2500 行数据\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf91abbe7f2e4e338e57318ad9057632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "预测测试集: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始样本第一行: 'R05K5826G4.jpg\t'\n",
      "预测完成! 结果保存为 submission_ensemble.csv，共 2500 个样本\n",
      "使用分隔符: ','\n",
      "生成文件的第一行: 'R05K5826G4.jpg,20 24 532 25 1043 28 1555 29 2066 32 2577 34 3089 35 3599 38 4110 41 4621 43 5133 44 5644 46 6155 48 6667 48 7178 50 7690 50 8201 52 8712 54 9224 54 9735 56 10247 57 10758 60 11269 67 11780 84 12292 88 12803 91 13314 96 13826 102 14338 115 14850 121 15362 128 15873 136 16385 145 16897 151 17409 158 17921 165 18433 173 18945 181 19457 183 19969 187 20481 199 20993 202 21505 206 22017 216 22529 223 23041 232 23553 240 24065 247 24577 250 25089 253 25601 258 26113 271 26625 277 27137 283 27649 287 28161 293 28673 297 29185 300 29697 304 30209 307 30721 101 30824 206 31233 100 31337 209 31745 100 31850 210 32257 99 32362 214 32769 99 32875 219 33281 99 33388 221 33797 50 33855 37 33901 223 34310 46 34369 35 34415 223 34825 40 34883 33 34932 220 35338 37 35396 32 35449 219 35853 7 35868 18 35910 30 35966 219 36382 15 36423 29 36482 218 36896 12 36935 29 37001 214 37448 28 37519 211 37961 27 38035 14 38052 194 38474 25 38554 6 38564 197 38987 24 39076 200 39501 21 39589 203 39937 1 40015 19 40102 204 40449 3 40530 15 40614 208 40961 5 41043 13 41128 212 41473 6 41557 10 41643 212 41985 7 42157 213 42497 8 42674 211 43009 9 43188 211 43521 11 43702 212 44033 12 44217 19 44244 186 44545 13 44735 11 44765 180 45057 15 45287 174 45569 16 45803 173 46081 17 46322 169 46593 19 46837 169 47105 20 47351 169 47617 21 47864 171 48129 23 48376 42 48425 125 48641 24 48889 21 48941 124 49153 25 49402 14 49457 124 49665 26 49915 11 49971 125 50177 27 50428 7 50485 127 50689 29 50942 1 50999 127 51201 31 51511 130 51713 36 52024 132 52225 43 52536 134 52737 50 53049 134 53249 53 53562 134 53761 55 54075 134 54273 56 54589 9 54606 116 54785 56 55122 113 55297 59 55637 110 55809 66 55881 2 56153 107 56321 80 56667 105 56833 87 57182 103 57345 93 57697 100 57857 99 58212 98 58369 102 58485 8 58727 95 58881 109 58992 15 59242 93 59393 128 59756 91 59905 129 60152 4 60270 89 60417 132 60662 8 60785 87 60929 136 61172 12 61298 86 61441 141 61682 14 61811 19 61835 62 61953 146 62193 16 62323 17 62351 58 62465 150 62620 8 62643 4 62704 17 62836 15 62867 54 62977 189 63215 19 63349 13 63383 50 63489 193 63727 19 63862 11 63897 49 64001 195 64238 21 64379 3 64412 46 64513 197 64750 21 64926 44 65025 199 65261 23 65439 43 65537 203 65773 23 65951 44 66049 209 66284 25 66464 43 66561 216 66796 26 66976 43 67073 220 67306 30 67488 43 67585 265 68000 43 68097 269 68513 43 68609 274 69026 42 69121 277 69539 41 69633 278 70051 42 70145 279 70564 41 70657 280 71077 40 71175 274 71589 41 71690 271 72101 41 72205 268 72613 42 72719 266 73125 42 73233 264 73637 42 73747 262 74150 42 74262 259 74662 42 74777 256 75059 8 75174 43 75292 254 75570 11 75687 42 75806 252 76081 13 76200 42 76321 251 76593 13 76712 42 76835 14 76861 227 77104 14 77224 43 77353 4 77376 229 77615 16 77736 43 77892 251 78248 44 78416 239 78760 44 78936 232 79273 44 79453 229 79729 6 79784 45 79967 230 80240 10 80296 45 80486 230 80751 13 80808 46 81005 226 81262 14 81319 47 81522 231 81773 15 81830 49 82043 229 82284 17 82342 49 82560 253 82854 49 83077 248 83365 51 83599 239 83877 51 84126 225 84389 51 84644 221 84901 51 85163 219 85413 52 85681 218 85924 53 86203 214 86436 53 86724 209 86946 56 87242 206 87455 59 87759 268 88274 265 88791 260 89309 255 89824 252 90339 249 90857 243 91380 232 91899 225 92416 220 92940 207 93461 198 93980 190 94500 181 95019 174 95539 165 96057 158 96577 150 97093 145 97612 137 98130 130 98649 122 99173 109 99695 97 100212 91 100728 86 101243 82 101760 75 102276 70 102794 62 103311 56 103827 51 104346 43 104864 36 105384 26 105908 11 137780 11 138291 18 138803 25 139315 33 139827 38 140338 48 140850 55 141362 61 141874 69 142386 74 142897 78 143409 80 143921 81 144433 82 144945 82 145457 84 145969 90 146481 92 146992 97 147504 104 148016 105 148528 106 149040 108 149551 110 150063 112 150575 113 151087 115 151599 115 152111 116 152623 117 153135 118 153646 120 154158 121 154361 3 154670 122 154870 15 155182 123 155351 10 155381 22 155694 123 155862 17 155893 31 156206 124 156373 21 156405 38 156718 125 156885 23 156916 43 157230 125 157397 24 157428 50 157742 126 157909 25 157940 54 157995 6 158254 126 158420 26 158452 68 158766 126 158932 26 158964 71 159278 126 159444 26 159475 74 159791 125 159956 26 159987 76 160304 124 160468 26 160499 78 160818 122 160980 25 161011 80 161335 103 161440 11 161492 25 161523 83 161853 68 161925 23 161955 7 162004 25 162034 87 162372 60 162442 16 162515 26 162546 89 162892 51 162959 9 163027 25 163058 90 163408 46 163473 5 163539 25 163570 91 163927 39 164051 25 164081 92 164443 35 164563 25 164593 92 164963 27 165074 26 165105 92 165486 16 165586 25 165617 92 166006 7 166098 25 166129 92 166610 25 166641 92 167122 25 167154 91 167633 26 167669 2 167680 77 168145 25 168212 20 168238 31 168657 25 168733 8 168763 16 169169 25 169281 7 169681 25 170193 25 170704 26 171216 25 171728 25 172240 25 172752 25 173265 24 173777 23 174289 23 174802 22 175315 20 175829 17 176348 9'\n",
      "生成的提交文件包含 2500 行\n",
      "行数检查通过!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 包含现有模型并训练新模型\n",
    "    #score = execute_ensemble_pipeline(\n",
    "    #    train_new=False,       # 是否训练新模型\n",
    "    #    include_existing=True  # 是否包含现有模型\n",
    "    #)\n",
    "    #print(f\"最终集成模型的Dice分数: {score:.4f}\")\n",
    "    predict_test_with_ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f277d-b717-4ef5-af02-6030746b8e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

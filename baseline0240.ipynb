{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35311bb3-c097-4f71-91fb-1e97cf866fd1",
   "metadata": {},
   "source": [
    "# 天池地表建筑物识别 Baseline\n",
    "\n",
    "这个 Notebook 实现了一个完整的训练和预测工作流。\n",
    "它使用了 `segmentation-models-pytorch` 库来构建带有预训练编码器的强大模型，\n",
    "使用混合精度训练 (Mixed Precision) 来提高效率，并根据 `test_sample_submit.csv`\n",
    "文件指定的顺序来正确处理测试数据集，以生成符合要求的提交文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b4fe49-4790-44ef-a383-0002c350469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本: 2.1.0\n",
      "Segmentation Models Pytorch 版本: 0.4.0\n",
      "Scikit-learn 版本: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "# === 1. 导入必要的库 ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib, sys, os, random, time, gc\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm # 用于显示进度条\n",
    "import matplotlib.pyplot as plt # 用于绘图\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 忽略不必要的警告信息\n",
    "\n",
    "# 数据增强库\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# PyTorch 核心库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torch.cuda.amp as amp # 用于混合精度训练\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau # 学习率调度器\n",
    "\n",
    "# Segmentation Models PyTorch 库\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# --- 新增：导入 KFold ---\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"Segmentation Models Pytorch 版本: {smp.__version__}\")\n",
    "# 打印 sklearn 版本（如果安装了）\n",
    "try:\n",
    "    import sklearn\n",
    "    print(f\"Scikit-learn 版本: {sklearn.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"警告: Scikit-learn (sklearn) 未安装。KFold 将无法使用。\")\n",
    "    print(\"请运行: pip install scikit-learn\")\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47fef401-2adc-408d-9ee3-4e1df412927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置参数已加载:\n",
      "  输出目录: output/\n",
      "  K-Fold 折数: 5\n",
      "  每折训练轮数: 15, 批大小: 16, 图片尺寸: 512\n",
      "  模型架构: Unet, 编码器: efficientnet-b3\n",
      "  运行设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# === 2. 配置参数 ===\n",
    "\n",
    "# --- 文件与目录路径 ---\n",
    "TRAIN_IMAGE_DIR = '数据集/train/' # 训练图片目录\n",
    "TRAIN_MASK_CSV = '数据集/train_mask.csv' # 训练集 RLE 编码文件\n",
    "TEST_IMAGE_DIR = '数据集/test/' # 测试图片目录\n",
    "SAMPLE_SUBMISSION_CSV = '数据集/test_sample_submit.csv' # 提交样例文件 (用于保证测试集顺序!)\n",
    "OUTPUT_DIR = 'output/' # 输出目录 (用于保存模型、日志、提交文件等)\n",
    "# BEST_MODEL_NAME 不再是单个文件，而是每个 fold 一个\n",
    "SUBMISSION_FILENAME = 'submission_kfold.csv' # 最终提交文件名 (区分开)\n",
    "\n",
    "# --- 训练超参数 ---\n",
    "SEED = 42 # 随机种子，保证结果可复现\n",
    "EPOCHS = 15 # **每折**训练的轮数 (根据需要调整)\n",
    "BATCH_SIZE = 16 # 每批处理的图片数量 (根据 GPU 显存大小调整)\n",
    "IMAGE_SIZE = 512 # 输入模型的图片尺寸 (训练和预测时都使用)\n",
    "LEARNING_RATE = 1e-4 # 初始学习率\n",
    "WEIGHT_DECAY = 1e-4 # 优化器的权重衰减\n",
    "THRESHOLD = 0.5 # 将模型输出概率转换为二值掩码 (0或1) 的阈值\n",
    "ACCUMULATION_STEPS = 8 # 梯度累积步数 (实际生效的批大小 = BATCH_SIZE * ACCUMULATION_STEPS)\n",
    "N_FOLDS = 5 # --- 新增：K 折交叉验证的折数 ---\n",
    "\n",
    "# --- 模型配置 ---\n",
    "# 可选的模型架构和编码器请参考: https://github.com/qubvel/segmentation_models.pytorch\n",
    "MODEL_ARC = 'Unet' # 例如: Unet, UnetPlusPlus, FPN, DeepLabV3Plus\n",
    "ENCODER = 'efficientnet-b3' # 例如: efficientnet-b3, resnet34, mobilenet_v2\n",
    "ENCODER_WEIGHTS = 'imagenet' # 使用在 ImageNet 上预训练的权重\n",
    "\n",
    "# --- 硬件设置 ---\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # 自动选择 GPU 或 CPU\n",
    "\n",
    "# --- 确保输出目录存在 ---\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"配置参数已加载:\")\n",
    "print(f\"  输出目录: {OUTPUT_DIR}\")\n",
    "print(f\"  K-Fold 折数: {N_FOLDS}\")\n",
    "print(f\"  每折训练轮数: {EPOCHS}, 批大小: {BATCH_SIZE}, 图片尺寸: {IMAGE_SIZE}\")\n",
    "print(f\"  模型架构: {MODEL_ARC}, 编码器: {ENCODER}\")\n",
    "print(f\"  运行设备: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ade3cbb6-1a7d-4597-8013-0fe24c2a0ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已设置随机种子: 42\n"
     ]
    }
   ],
   "source": [
    "# === 3. 辅助函数 ===\n",
    "\n",
    "# --- 设置随机种子 ---\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # 如果使用多块 GPU\n",
    "        # 以下两行可能提高性能，但可能导致结果不完全可复现\n",
    "        # torch.backends.cudnn.deterministic = False\n",
    "        # torch.backends.cudnn.benchmark = True\n",
    "        # 为了更好的可复现性 (可能会牺牲一点性能):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(SEED)\n",
    "print(f\"已设置随机种子: {SEED}\")\n",
    "\n",
    "# --- RLE 编码/解码函数 (来自题目描述) ---\n",
    "def rle_encode(im):\n",
    "    '''将二值掩码图片编码为 RLE 字符串'''\n",
    "    pixels = im.flatten(order = 'F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(512, 512)):\n",
    "    '''将 RLE 字符串解码为二值掩码图片'''\n",
    "    if mask_rle == '' or pd.isna(mask_rle): # 处理空 RLE 或 NaN 值\n",
    "        return np.zeros(shape, dtype=np.uint8)\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order='F')\n",
    "\n",
    "# --- 清理 GPU 显存 ---\n",
    "def clear_memory():\n",
    "    if DEVICE == 'cuda':\n",
    "        torch.cuda.empty_cache() # 释放未被使用的缓存显存\n",
    "    gc.collect() # 执行 Python 的垃圾回收"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16789218-1524-4273-a5b3-7376f0be8941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据增强流程已定义。\n"
     ]
    }
   ],
   "source": [
    "# === 4. 数据增强策略 ===\n",
    "\n",
    "# --- 训练集数据增强 ---\n",
    "# 包含一些常用的几何和颜色变换\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE), # 调整图片大小\n",
    "    A.HorizontalFlip(p=0.5), # 水平翻转\n",
    "    A.VerticalFlip(p=0.5), # 垂直翻转\n",
    "    A.RandomRotate90(p=0.5), # 随机旋转90度\n",
    "    # 轻微的仿射变换 (位移、缩放、旋转)\n",
    "    A.ShiftScaleRotate(p=0.5, shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "    # 可选的颜色变换 (如果需要可以取消注释)\n",
    "    # A.RandomBrightnessContrast(p=0.3), # 随机调整亮度和对比度\n",
    "    # A.HueSaturationValue(p=0.3), # 随机调整色调、饱和度、明度\n",
    "    # 归一化 (使用 ImageNet 的均值和标准差)\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(), # 转换为 PyTorch Tensor\n",
    "])\n",
    "\n",
    "# --- 验证集/测试集数据增强 ---\n",
    "# 通常只包含 Resize 和 Normalize\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "print(\"数据增强流程已定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e9f672-0b09-4153-833b-ea9bd5936630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 类已定义。\n"
     ]
    }
   ],
   "source": [
    "# === 5. Dataset 类定义 ===\n",
    "# 已修改，使其在处理测试集时能返回图片和对应的文件名\n",
    "\n",
    "class BuildingSegmentationDataset(D.Dataset):\n",
    "    def __init__(self, img_paths, mask_info=None, transform=None, is_test=False):\n",
    "        \"\"\"\n",
    "        初始化 Dataset 对象。\n",
    "        Args:\n",
    "            img_paths (list): 图片文件的完整路径列表。\n",
    "            mask_info (pd.DataFrame or dict, optional): 包含掩码信息的对象。\n",
    "                训练/验证时: 可以是包含 'name' 和 'mask' 列的 DataFrame，或 {文件名: RLE字符串} 的字典。\n",
    "                测试时: 应为 None。\n",
    "            transform (callable, optional): Albumentations 定义的数据增强/转换流程。\n",
    "            is_test (bool): 如果是 True，表示这是测试数据集。\n",
    "        \"\"\"\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_info = mask_info\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "        # 如果不是测试集，并且提供了 mask_info，则创建一个快速查找 RLE 的字典\n",
    "        if not self.is_test and self.mask_info is not None:\n",
    "             if isinstance(self.mask_info, pd.DataFrame):\n",
    "                 # 确保 'name' 列存在，并将其设为索引以加速查找\n",
    "                 if 'name' in self.mask_info.columns:\n",
    "                     # 使用 DataFrame 的 set_index 和 to_dict 方法创建查找字典\n",
    "                     self.mask_lookup = self.mask_info.set_index('name')['mask'].to_dict()\n",
    "                 else:\n",
    "                     print(\"警告: 在 mask_info DataFrame 中未找到 'name' 列。\")\n",
    "                     self.mask_lookup = {}\n",
    "             elif isinstance(self.mask_info, dict):\n",
    "                 # 如果 mask_info 本身就是字典，直接使用\n",
    "                 self.mask_lookup = self.mask_info\n",
    "             else:\n",
    "                 # 处理未预期的 mask_info 类型\n",
    "                 print(f\"警告: mask_info 的类型未预期: {type(self.mask_info)}\")\n",
    "                 self.mask_lookup = {}\n",
    "        else:\n",
    "             self.mask_lookup = {} # 测试集不需要 RLE 查找\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集中样本的总数\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 根据索引获取单个样本\n",
    "        img_path = self.img_paths[idx]\n",
    "        filename = os.path.basename(img_path) # 从完整路径中提取文件名\n",
    "\n",
    "        # 加载图片\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: # 检查图片是否成功加载\n",
    "                raise IOError(f\"无法加载图片: {img_path}\")\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # OpenCV 默认是 BGR，转换为 RGB\n",
    "        except Exception as e:\n",
    "            print(f\"加载图片 {img_path} 时出错: {e}\")\n",
    "            # 如果加载失败，创建一个黑色图片作为替代，以避免程序崩溃\n",
    "            img = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8) # 使用配置的尺寸\n",
    "            if self.is_test:\n",
    "                 # 对黑色图片应用变换 (主要是 Resize, Normalize, ToTensor)\n",
    "                if self.transform:\n",
    "                    transformed = self.transform(image=img)\n",
    "                    img = transformed['image']\n",
    "                return img, filename # 返回处理后的黑色图片和文件名\n",
    "            else:\n",
    "                 # 训练/验证时，还需要返回一个空的掩码\n",
    "                 mask = np.zeros((IMAGE_SIZE, IMAGE_SIZE), dtype=np.uint8)\n",
    "                 if self.transform:\n",
    "                     transformed = self.transform(image=img, mask=mask)\n",
    "                     img = transformed['image']\n",
    "                     # 确保掩码是 [1, H, W] 的 float Tensor\n",
    "                     mask = transformed['mask'].unsqueeze(0).float()\n",
    "                 return img, mask\n",
    "\n",
    "        # --- 处理逻辑分支 ---\n",
    "        if self.is_test:\n",
    "            # 如果是测试集，只对图片应用变换\n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=img)\n",
    "                img = transformed['image']\n",
    "            # 返回处理后的图片和文件名\n",
    "            return img, filename\n",
    "        else:\n",
    "            # 如果是训练集或验证集，需要加载并处理掩码\n",
    "            mask_rle = self.mask_lookup.get(filename, '') # 使用文件名查找对应的 RLE 字符串\n",
    "            mask = rle_decode(mask_rle, shape=(512, 512)) # 解码 RLE，注意使用原始尺寸 (512x512)\n",
    "\n",
    "            # 对图片和掩码同时应用变换\n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=img, mask=mask)\n",
    "                img = transformed['image']\n",
    "                mask = transformed['mask'] # 变换后的掩码\n",
    "\n",
    "            # 将掩码转换为 PyTorch Tensor，确保是 float 类型，并增加一个通道维度\n",
    "            if isinstance(mask, np.ndarray):\n",
    "                mask = torch.from_numpy(mask) # 从 NumPy 数组转换为 Tensor\n",
    "\n",
    "            # 增加通道维度 [H, W] -> [1, H, W]，并确保是 float 类型\n",
    "            mask = mask.unsqueeze(0).float()\n",
    "\n",
    "            # 返回处理后的图片和掩码\n",
    "            return img, mask\n",
    "\n",
    "print(\"Dataset 类已定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51260fbe-c7fe-4a8b-ac72-4d6a3444ffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数已定义 (Dice, Focal, Combined)。\n"
     ]
    }
   ],
   "source": [
    "# === 6. 损失函数定义 ===\n",
    "\n",
    "# --- Dice Loss ---\n",
    "# 用于衡量预测掩码和真实掩码之间的重叠程度\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0): # smooth 参数用于防止分母为零\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, pred_logits, target):\n",
    "        # pred_logits 是模型的原始输出 (通常未经 sigmoid)\n",
    "        pred_prob = torch.sigmoid(pred_logits) # 将 logits 转换为概率 (0到1之间)\n",
    "        # 计算交集 (预测为1且真实为1的像素)\n",
    "        intersection = (pred_prob * target).sum(dim=(2,3)) # 在高度和宽度维度上求和\n",
    "        # 计算并集 (预测为1的像素 + 真实为1的像素)\n",
    "        union = pred_prob.sum(dim=(2,3)) + target.sum(dim=(2,3))\n",
    "        # 计算 Dice 系数\n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        # Dice Loss = 1 - Dice 系数 (目标是最小化 Loss，即最大化 Dice 系数)\n",
    "        return 1.0 - dice.mean() # 返回整个 batch 的平均 Dice Loss\n",
    "\n",
    "# --- Focal Loss ---\n",
    "# 用于处理类别不平衡问题，降低易分类样本的权重，关注难分类样本\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25): # gamma 控制调制因子，alpha 控制类别权重\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha # 正样本 (前景) 的权重\n",
    "        # 使用 BCEWithLogitsLoss 可以提高数值稳定性，它内部处理了 sigmoid\n",
    "        self.bce_with_logits = nn.BCEWithLogitsLoss(reduction='none') # 不进行自动平均\n",
    "\n",
    "    def forward(self, pred_logits, target):\n",
    "        # 计算标准的二元交叉熵损失 (每个像素单独计算)\n",
    "        bce_loss = self.bce_with_logits(pred_logits, target)\n",
    "        # 计算预测概率\n",
    "        pred_prob = torch.sigmoid(pred_logits)\n",
    "        # 计算 pt (预测正确的概率)\n",
    "        # 如果 target 是 1, pt = pred_prob; 如果 target 是 0, pt = 1 - pred_prob\n",
    "        pt = torch.where(target == 1, pred_prob, 1 - pred_prob)\n",
    "        # 计算 alpha 权重\n",
    "        # 如果 target 是 1, alpha_t = alpha; 如果 target 是 0, alpha_t = 1 - alpha\n",
    "        alpha_t = torch.where(target == 1, self.alpha, 1 - self.alpha)\n",
    "\n",
    "        # 计算 Focal Loss 的调制因子 (1 - pt)^gamma\n",
    "        focal_weight = alpha_t * (1 - pt).pow(self.gamma)\n",
    "        # 计算最终的 Focal Loss\n",
    "        focal_loss = focal_weight * bce_loss\n",
    "        # 返回整个 batch 和所有像素的平均 Focal Loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# --- Combined Loss (Dice + Focal) ---\n",
    "# 结合 Dice Loss 和 Focal Loss 的优点\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, dice_weight=0.5, focal_weight=0.5, focal_gamma=2.0, focal_alpha=0.25):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice_weight = dice_weight # Dice Loss 的权重\n",
    "        self.focal_weight = focal_weight # Focal Loss 的权重\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.focal_loss = FocalLoss(gamma=focal_gamma, alpha=focal_alpha)\n",
    "\n",
    "    def forward(self, pred_logits, target):\n",
    "        # 分别计算 Dice Loss 和 Focal Loss\n",
    "        dice = self.dice_loss(pred_logits, target)\n",
    "        focal = self.focal_loss(pred_logits, target)\n",
    "        # 按权重组合两种损失\n",
    "        return self.dice_weight * dice + self.focal_weight * focal\n",
    "\n",
    "print(\"损失函数已定义 (Dice, Focal, Combined)。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b12eb4b8-719a-4677-a941-5aa1f0332439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping 类已定义。\n"
     ]
    }
   ],
   "source": [
    "# === 7. Early Stopping (早停机制) ===\n",
    "# 用于在验证集性能不再提升时自动停止训练，防止过拟合\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"当验证集指标在一定轮数内不再改善时，提前停止训练。\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print, mode='min'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): 验证指标没有改善后，允许继续训练的轮数。\n",
    "            verbose (bool): 如果为 True，则打印每次指标改善的信息。\n",
    "            delta (float): 被认为是指标改善的最小变化量。\n",
    "            path (str): 保存最佳模型权重的文件路径。\n",
    "            trace_func (function): 用于打印日志信息的函数 (默认为 print)。\n",
    "            mode (str): 'min' 表示监控的指标越小越好 (如 Loss)，'max' 表示越大越好 (如 Dice Score)。\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0 # 记录指标没有改善的轮数\n",
    "        self.best_score = None # 记录迄今为止最好的指标分数\n",
    "        self.early_stop = False # 标记是否应该停止训练\n",
    "        self.val_metric_best = np.Inf if mode == 'min' else -np.Inf # 根据 mode 初始化最佳指标\n",
    "        self.delta = delta # 改善的阈值\n",
    "        self.path = path # 模型保存路径\n",
    "        self.trace_func = trace_func\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, val_metric, model):\n",
    "        # 将当前轮次的验证指标传入\n",
    "        score = val_metric\n",
    "\n",
    "        if self.best_score is None:\n",
    "            # 第一轮，直接记录分数并保存模型\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_metric, model)\n",
    "        elif (self.mode == 'min' and score >= self.best_score - self.delta) or \\\n",
    "             (self.mode == 'max' and score <= self.best_score + self.delta):\n",
    "            # 如果指标没有改善 (或改善小于 delta)\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping 计数: {self.counter} / {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                # 达到容忍轮数，标记停止\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            # 如果指标有改善\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_metric, model) # 保存更好的模型\n",
    "            self.counter = 0 # 重置计数器\n",
    "\n",
    "    def save_checkpoint(self, val_metric, model):\n",
    "        '''当验证指标改善时，保存模型权重。'''\n",
    "        if self.verbose:\n",
    "             # 计算改善量\n",
    "             improvement = self.val_metric_best - val_metric if self.mode == 'min' else val_metric - self.val_metric_best\n",
    "             self.trace_func(f'验证指标改善 ({self.val_metric_best:.6f} --> {val_metric:.6f}, 改善量: {improvement:.6f}). 保存模型 ...')\n",
    "        # 保存模型的状态字典\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        # 更新记录的最佳指标\n",
    "        self.val_metric_best = val_metric\n",
    "\n",
    "print(\"EarlyStopping 类已定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb1093a4-5655-494f-a9ea-840a3603050c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练和验证函数已定义。\n"
     ]
    }
   ],
   "source": [
    "# === 8. 定义训练和验证函数 ===\n",
    "\n",
    "# --- 单轮训练函数 ---\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler, accumulation_steps):\n",
    "    model.train() # 将模型设置为训练模式 (启用 Dropout 等)\n",
    "    total_loss = 0.0 # 记录该轮的总损失\n",
    "    optimizer.zero_grad() # 清空之前的梯度\n",
    "\n",
    "    # 使用 tqdm 显示进度条\n",
    "    progress_bar = tqdm(dataloader, desc=\"训练中\", leave=False)\n",
    "    for i, (images, masks) in enumerate(progress_bar):\n",
    "        # 将数据移动到指定设备 (GPU 或 CPU)\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device) # 应为 [B, 1, H, W] 且 float 类型\n",
    "\n",
    "        # 使用自动混合精度上下文管理器\n",
    "        with amp.autocast():\n",
    "            outputs = model(images) # 模型前向传播，得到 logits\n",
    "            loss = criterion(outputs, masks) # 计算损失\n",
    "            # 为了梯度累积，将损失除以累积步数\n",
    "            loss = loss / accumulation_steps\n",
    "\n",
    "        # 使用混合精度的 scaler 来缩放损失并进行反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 每 accumulation_steps 步或者在最后一个 batch 时，执行一次优化器步骤\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n",
    "            # 在梯度裁剪前，需要 unscale 梯度\n",
    "            scaler.unscale_(optimizer)\n",
    "            # 进行梯度裁剪，防止梯度爆炸 (可选但推荐)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            # 执行优化器步骤 (更新模型权重)\n",
    "            scaler.step(optimizer)\n",
    "            # 更新 scaler 的状态\n",
    "            scaler.update()\n",
    "            # 清空梯度，为下一次累积做准备\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # 记录当前 batch 的损失 (乘以累积步数还原)\n",
    "        batch_loss = loss.item() * accumulation_steps\n",
    "        total_loss += batch_loss\n",
    "        # 更新进度条显示当前 batch 的损失\n",
    "        progress_bar.set_postfix(loss=f'{batch_loss:.4f}')\n",
    "\n",
    "    # 计算该轮的平均损失 (按样本数计算)\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# --- 验证函数 ---\n",
    "@torch.no_grad() # 装饰器，表示在此函数内不计算梯度，节省显存和计算\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval() # 将模型设置为评估模式 (禁用 Dropout 等)\n",
    "    total_loss = 0.0 # 记录总验证损失\n",
    "    dice_scores = [] # 记录每个样本的 Dice 分数\n",
    "\n",
    "    # 使用 tqdm 显示进度条\n",
    "    progress_bar = tqdm(dataloader, desc=\"验证中\", leave=False)\n",
    "    for images, masks in progress_bar:\n",
    "        # 将数据移动到设备\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device) # 应为 [B, 1, H, W] 且 float 类型\n",
    "\n",
    "        # 使用混合精度进行前向传播 (即使不训练，也可以加速)\n",
    "        with amp.autocast():\n",
    "             outputs = model(images) # 模型输出 logits\n",
    "             loss = criterion(outputs, masks) # 计算损失\n",
    "\n",
    "        total_loss += loss.item() # 累加 batch 损失\n",
    "\n",
    "        # --- 计算 Dice 分数 ---\n",
    "        preds_prob = torch.sigmoid(outputs) # 转换为概率\n",
    "        # 使用阈值将概率图二值化为 0 或 1\n",
    "        preds_binary = (preds_prob > THRESHOLD).float()\n",
    "\n",
    "        # 计算 Dice 分数 (与 DiceLoss 类似，但针对每个样本计算)\n",
    "        # 在通道、高度、宽度维度上求和 (通道维度 C=1)\n",
    "        intersection = (preds_binary * masks).sum(dim=(1, 2, 3))\n",
    "        union = preds_binary.sum(dim=(1, 2, 3)) + masks.sum(dim=(1, 2, 3))\n",
    "        # 添加平滑项 epsilon 防止除以零\n",
    "        dice = (2.0 * intersection + 1e-7) / (union + 1e-7)\n",
    "        # 将当前 batch 中每个样本的 Dice 分数添加到列表中\n",
    "        dice_scores.extend(dice.cpu().numpy())\n",
    "\n",
    "    # 计算平均验证损失 (按 batch 数计算)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    # 计算所有验证样本的平均 Dice 分数\n",
    "    avg_dice = np.mean(dice_scores)\n",
    "    return avg_loss, avg_dice\n",
    "\n",
    "print(\"训练和验证函数已定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a74e227-6ded-417a-bf5c-fd4b339079cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 9. 主训练流程函数 (修改为 K 折交叉验证) ===\n",
    "\n",
    "def run_training_kfold():\n",
    "    print(f\"\\n===== 开始 {N_FOLDS}-折交叉验证训练 =====\")\n",
    "    clear_memory()\n",
    "    all_fold_best_model_paths = [] # 存储每一折最佳模型的路径\n",
    "\n",
    "    # --- 1. 加载完整的训练数据信息 ---\n",
    "    try:\n",
    "        print(\"加载训练数据信息...\")\n",
    "        # !! 注意分隔符 !!\n",
    "        train_mask_df_full = pd.read_csv(TRAIN_MASK_CSV, sep='\\t', names=['name', 'mask'])\n",
    "        train_mask_df_full['img_path'] = train_mask_df_full['name'].apply(lambda x: os.path.join(TRAIN_IMAGE_DIR, x))\n",
    "        print(f\"找到 {len(train_mask_df_full)} 条训练数据记录。\")\n",
    "        # 检查图片路径\n",
    "        sample_paths = train_mask_df_full['img_path'].sample(min(5, len(train_mask_df_full))).tolist()\n",
    "        missing_samples = [p for p in sample_paths if not os.path.exists(p)]\n",
    "        if missing_samples:\n",
    "             print(f\"警告: 无法找到以下示例图片文件: {missing_samples}\")\n",
    "        else:\n",
    "             print(\"示例图片路径检查通过。\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载 {TRAIN_MASK_CSV} 时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 2. 初始化 KFold ---\n",
    "    # shuffle=True 保证每次运行 KFold 前数据被打乱\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    # --- 3. K-Fold 循环 ---\n",
    "    for fold in range(N_FOLDS):\n",
    "        print(f\"\\n--- 开始训练第 {fold+1}/{N_FOLDS} 折 ---\")\n",
    "        clear_memory()\n",
    "\n",
    "        # --- 3.1 获取当前折的训练和验证索引 ---\n",
    "        # kf.split 返回的是索引\n",
    "        train_idx, valid_idx = list(kf.split(train_mask_df_full))[fold]\n",
    "        train_df = train_mask_df_full.iloc[train_idx].reset_index(drop=True)\n",
    "        valid_df = train_mask_df_full.iloc[valid_idx].reset_index(drop=True)\n",
    "        print(f\"第 {fold+1} 折数据划分: {len(train_df)} 训练样本, {len(valid_df)} 验证样本\")\n",
    "\n",
    "        # --- 3.2 创建当前折的 Dataset 和 DataLoader ---\n",
    "        print(f\"第 {fold+1} 折: 创建 Dataset 和 DataLoader...\")\n",
    "        train_dataset = BuildingSegmentationDataset(\n",
    "            img_paths=train_df['img_path'].tolist(),\n",
    "            mask_info=train_df[['name', 'mask']],\n",
    "            transform=train_transform,\n",
    "            is_test=False\n",
    "        )\n",
    "        valid_dataset = BuildingSegmentationDataset(\n",
    "            img_paths=valid_df['img_path'].tolist(),\n",
    "            mask_info=valid_df[['name', 'mask']],\n",
    "            transform=valid_transform,\n",
    "            is_test=False\n",
    "        )\n",
    "        \"\"\"train_loader = D.DataLoader(\n",
    "            train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "            num_workers=max(1, os.cpu_count() // 2), pin_memory=True, drop_last=True\n",
    "        )\n",
    "        valid_loader = D.DataLoader(\n",
    "            valid_dataset, batch_size=BATCH_SIZE * 2, shuffle=False,\n",
    "            num_workers=max(1, os.cpu_count() // 2), pin_memory=True\n",
    "        )\"\"\"\n",
    "        train_loader = D.DataLoader(\n",
    "            train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "            num_workers=2, pin_memory=True, drop_last=True\n",
    "        )\n",
    "        valid_loader = D.DataLoader(\n",
    "            valid_dataset, batch_size=BATCH_SIZE * 2, shuffle=False,\n",
    "            num_workers=2, pin_memory=True\n",
    "        )\n",
    "        print(f\"第 {fold+1} 折: DataLoader 创建完成。\")\n",
    "\n",
    "        # --- 3.3 初始化当前折的模型、损失、优化器、调度器 ---\n",
    "        print(f\"第 {fold+1} 折: 初始化模型及相关组件...\")\n",
    "        # !! 每次循环都重新创建模型，保证每折独立训练 !!\n",
    "        model = smp.create_model(\n",
    "            arch=MODEL_ARC, encoder_name=ENCODER, encoder_weights=ENCODER_WEIGHTS,\n",
    "            in_channels=3, classes=1,\n",
    "        ).to(DEVICE)\n",
    "        criterion = CombinedLoss(dice_weight=0.6, focal_weight=0.4)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        # Scheduler T_max 仍然是每折的总轮数\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE / 100)\n",
    "        scaler = amp.GradScaler()\n",
    "        # Early Stopping 保存路径包含折数\n",
    "        fold_model_path = os.path.join(OUTPUT_DIR, f\"best_model_fold_{fold}.pth\")\n",
    "        early_stopping = EarlyStopping(patience=7, verbose=True, path=fold_model_path, mode='max', delta=0.001)\n",
    "        print(f\"第 {fold+1} 折: 初始化完成。\")\n",
    "\n",
    "        # --- 3.4 当前折的训练循环 ---\n",
    "        print(f\"第 {fold+1} 折: 开始训练循环...\")\n",
    "        best_fold_val_dice = -np.inf\n",
    "        fold_history = {'train_loss': [], 'val_loss': [], 'val_dice': [], 'lr': []}\n",
    "        log_file_path = os.path.join(OUTPUT_DIR, f\"training_log_fold_{fold}.csv\") # 每折一个日志\n",
    "\n",
    "        with open(log_file_path, \"w\") as log_file:\n",
    "             log_file.write(\"epoch,train_loss,val_loss,val_dice,learning_rate\\n\")\n",
    "             for epoch in range(1, EPOCHS + 1):\n",
    "                 start_time = time.time()\n",
    "                 avg_train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE, scaler, ACCUMULATION_STEPS)\n",
    "                 clear_memory()\n",
    "                 avg_val_loss, avg_val_dice = validate(model, valid_loader, criterion, DEVICE)\n",
    "                 clear_memory()\n",
    "\n",
    "                 current_lr = optimizer.param_groups[0]['lr']\n",
    "                 scheduler.step() # CosineAnnealingLR\n",
    "\n",
    "                 elapsed_time = time.time() - start_time\n",
    "                 fold_history['train_loss'].append(avg_train_loss)\n",
    "                 fold_history['val_loss'].append(avg_val_loss)\n",
    "                 fold_history['val_dice'].append(avg_val_dice)\n",
    "                 fold_history['lr'].append(current_lr)\n",
    "\n",
    "                 log_line = f\"{epoch},{avg_train_loss:.6f},{avg_val_loss:.6f},{avg_val_dice:.6f},{current_lr:.8f}\\n\"\n",
    "                 log_file.write(log_line)\n",
    "                 log_file.flush()\n",
    "\n",
    "                 print(f\"Fold {fold+1}/{N_FOLDS} - Epoch {epoch}/{EPOCHS} - \"\n",
    "                       f\"Time: {elapsed_time:.0f}s - LR: {current_lr:.6f} - \"\n",
    "                       f\"Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f} - Val Dice: {avg_val_dice:.4f}\")\n",
    "\n",
    "                 early_stopping(avg_val_dice, model)\n",
    "                 if early_stopping.early_stop:\n",
    "                     print(f\"第 {fold+1} 折触发 Early Stopping!\")\n",
    "                     break\n",
    "                 if avg_val_dice > best_fold_val_dice:\n",
    "                     best_fold_val_dice = avg_val_dice\n",
    "\n",
    "        print(f\"--- 第 {fold+1}/{N_FOLDS} 折训练结束。最佳验证 Dice: {early_stopping.best_score:.4f} ---\")\n",
    "        # 将当前折保存的最佳模型路径添加到列表中\n",
    "        if os.path.exists(fold_model_path):\n",
    "            all_fold_best_model_paths.append(fold_model_path)\n",
    "        else:\n",
    "            print(f\"警告: 第 {fold+1} 折的最佳模型文件 {fold_model_path} 未找到！\")\n",
    "\n",
    "        # --- 可选：绘制当前折的训练曲线 ---\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(fold_history['train_loss'], label='Train Loss')\n",
    "        plt.plot(fold_history['val_loss'], label='Val Loss')\n",
    "        plt.title(f'Fold {fold+1} Loss')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(fold_history['val_dice'], label='Val Dice')\n",
    "        plt.title(f'Fold {fold+1} Validation Dice')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('Dice Score'); plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'training_history_fold_{fold}.png'))\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"\\n===== {N_FOLDS}-折交叉验证训练全部结束 =====\")\n",
    "    print(f\"共保存了 {len(all_fold_best_model_paths)} 个模型:\")\n",
    "    for p in all_fold_best_model_paths:\n",
    "        print(f\"  - {p}\")\n",
    "\n",
    "    # 返回所有保存的模型路径列表\n",
    "    return all_fold_best_model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2d7e69f-f071-42a2-a957-d595506cb426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 10. 测试集预测函数 (K-Fold 集成, 文件名清理, 使用内建 csv 模块写入, 无表头) ===\n",
    "# 假设以下变量已在之前的单元格定义:\n",
    "# SAMPLE_SUBMISSION_CSV, TEST_IMAGE_DIR, OUTPUT_DIR, SUBMISSION_FILENAME\n",
    "# MODEL_ARC, ENCODER, DEVICE, BATCH_SIZE, N_FOLDS\n",
    "# valid_transform, rle_encode, clear_memory, THRESHOLD, BuildingSegmentationDataset\n",
    "\n",
    "@torch.no_grad() # 预测时不需要计算梯度\n",
    "def predict_test_set_kfold(model_paths: list): # 接收模型路径列表\n",
    "    print(\"\\n===== 开始对测试集进行 K-Fold 集成预测 (无表头输出, 使用内建 csv 模块) =====\")\n",
    "    if not model_paths:\n",
    "        print(\"错误：没有提供模型路径列表，无法进行预测。\")\n",
    "        return\n",
    "    clear_memory()\n",
    "\n",
    "    # --- 1. 加载测试数据路径和顺序 (包含文件名清理) ---\n",
    "    try:\n",
    "        print(f\"从 {SAMPLE_SUBMISSION_CSV} 加载测试集文件顺序...\")\n",
    "        sample_df = pd.read_csv(SAMPLE_SUBMISSION_CSV, sep='\\t', names=['name', 'mask'])\n",
    "        test_filenames_raw = sample_df['name'].tolist()\n",
    "        print(f\"{len(test_filenames_raw)} 个原始测试文件名加载成功。\")\n",
    "\n",
    "        print(\"清理文件名中的空白字符...\")\n",
    "        test_filenames = [fname.strip() for fname in test_filenames_raw]\n",
    "        print(f\"清理后的第一个文件名示例: '{test_filenames[0]}'\")\n",
    "\n",
    "        test_img_paths = [os.path.join(TEST_IMAGE_DIR, fname) for fname in test_filenames]\n",
    "\n",
    "        print(\"使用清理后的路径进行示例检查...\")\n",
    "        sample_paths = random.sample(test_img_paths, min(5, len(test_img_paths)))\n",
    "        missing_samples = [p for p in sample_paths if not os.path.exists(p)]\n",
    "        if missing_samples:\n",
    "            print(f\"警告: 清理后仍然无法找到示例测试图片: {missing_samples}\")\n",
    "        else:\n",
    "            print(\"清理后的示例测试图片路径检查通过。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"加载或处理 {SAMPLE_SUBMISSION_CSV} 时出错: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. 创建测试 Dataset 和 DataLoader ---\n",
    "    print(\"创建测试集 Dataset 和 DataLoader...\")\n",
    "    test_dataset = BuildingSegmentationDataset(\n",
    "        img_paths=test_img_paths, mask_info=None, transform=valid_transform, is_test=True\n",
    "    )\n",
    "    test_loader = D.DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=2, pin_memory=False\n",
    "    )\n",
    "    print(\"测试集 DataLoader 创建完成。\")\n",
    "\n",
    "\n",
    "    # --- 3. 加载所有 K 折的模型 ---\n",
    "    print(f\"加载 {len(model_paths)} 个 K-Fold 模型...\")\n",
    "    models = []\n",
    "    for model_path in model_paths:\n",
    "        print(f\"  加载: {model_path}\")\n",
    "        model = smp.create_model(\n",
    "            arch=MODEL_ARC, encoder_name=ENCODER, encoder_weights=None, in_channels=3, classes=1,\n",
    "        )\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "            model.to(DEVICE)\n",
    "            model.eval()\n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"  加载模型 {model_path} 失败: {e}\")\n",
    "    if not models:\n",
    "        print(\"错误：未能成功加载任何模型，无法进行预测。\")\n",
    "        return\n",
    "    print(f\"成功加载 {len(models)} 个模型。\")\n",
    "\n",
    "\n",
    "    # --- 4. 执行集成预测 ---\n",
    "    print(\"开始集成预测...\")\n",
    "    results_list = []\n",
    "    progress_bar = tqdm(test_loader, desc=\"集成预测中\")\n",
    "\n",
    "    for images, filenames_batch in progress_bar:\n",
    "        images = images.to(DEVICE)\n",
    "        batch_probs_sum = torch.zeros_like(images[:, 0:1, :, :]).float().to(DEVICE)\n",
    "\n",
    "        for model in models:\n",
    "            with amp.autocast():\n",
    "                outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            batch_probs_sum += probs\n",
    "\n",
    "        avg_probs = batch_probs_sum / len(models)\n",
    "\n",
    "        for i in range(avg_probs.shape[0]):\n",
    "            prob_single = avg_probs[i].squeeze().cpu().numpy()\n",
    "            raw_filename = filenames_batch[i]\n",
    "            clean_filename = raw_filename.strip()\n",
    "\n",
    "            mask_resized = cv2.resize(prob_single, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
    "            mask_binary = (mask_resized > THRESHOLD).astype(np.uint8)\n",
    "            rle = rle_encode(mask_binary)\n",
    "\n",
    "            # 可选的额外清理：显式移除 RLE 中的换行符 (如果需要)\n",
    "            # rle_sanitized = rle.replace('\\n', ' ').replace('\\r', '')\n",
    "            # results_list.append([clean_filename, rle_sanitized])\n",
    "            results_list.append([clean_filename, rle]) # 暂时先不用额外清理\n",
    "\n",
    "    print(f\"集成预测完成。共生成 {len(results_list)} 条结果。\")\n",
    "\n",
    "\n",
    "    # --- 5. 创建并保存提交文件 (使用内建 csv 模块, 无表头) ---\n",
    "    print(\"创建提交文件 (无表头, 使用内建 csv 模块)...\")\n",
    "    if len(results_list) != len(test_filenames):\n",
    "         print(f\"警告: 生成的结果数量 ({len(results_list)}) 与预期的测试文件数量 ({len(test_filenames)}) 不符！\")\n",
    "\n",
    "    submission_map = {name: rle for name, rle in results_list}\n",
    "    ordered_results = []\n",
    "    for fname in test_filenames:\n",
    "         rle_str = submission_map.get(fname, '')\n",
    "         ordered_results.append([fname, rle_str]) # 创建列表的列表\n",
    "\n",
    "    # 保存\n",
    "    submission_path = os.path.join(OUTPUT_DIR, SUBMISSION_FILENAME)\n",
    "    try:\n",
    "        print(f\"正在使用 Python 内建 csv 模块保存提交文件到: {submission_path} (无表头)...\")\n",
    "        # 使用 'w' 模式打开文件，newline='' 防止 csv 模块写入多余的空行\n",
    "        with open(submission_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            # 创建 csv writer 对象，指定逗号分隔，并对非数字加引号\n",
    "            writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_NONNUMERIC)\n",
    "            # 一次性写入所有行数据 (ordered_results 是一个列表的列表)\n",
    "            writer.writerows(ordered_results)\n",
    "\n",
    "        print(f\"提交文件已成功保存到: {submission_path}\")\n",
    "        print(\"提交文件前 5 行预览 (注意可能有引号):\")\n",
    "        try:\n",
    "            with open(submission_path, 'r', encoding='utf-8') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if i < 5:\n",
    "                        print(line.strip())\n",
    "                    else:\n",
    "                        break\n",
    "        except Exception as read_e:\n",
    "            print(f\"无法预览文件内容: {read_e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"使用内建 csv 模块保存提交文件时出错: {e}\")\n",
    "\n",
    "    clear_memory()\n",
    "\n",
    "# --- 提示 ---\n",
    "# 1. 确保在这个单元格运行前，所有依赖项已定义。\n",
    "# 2. 确保 `import csv` 已执行。\n",
    "# 3. 运行调用此函数的代码来执行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52b790b4-13b1-4bae-b1e2-98af611bb1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 开始 5-折交叉验证训练 =====\n",
      "加载训练数据信息...\n",
      "找到 30000 条训练数据记录。\n",
      "示例图片路径检查通过。\n",
      "\n",
      "--- 开始训练第 1/5 折 ---\n",
      "第 1 折数据划分: 24000 训练样本, 6000 验证样本\n",
      "第 1 折: 创建 Dataset 和 DataLoader...\n",
      "第 1 折: DataLoader 创建完成。\n",
      "第 1 折: 初始化模型及相关组件...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# === 11. 主执行入口 ===\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# --- 执行 K-Fold 训练 ---\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# run_training_kfold() 返回一个包含所有折数最佳模型路径的列表\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     list_of_best_models \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training_kfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# --- 执行 K-Fold 集成预测 ---\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m list_of_best_models \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_best_models) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[20], line 77\u001b[0m, in \u001b[0;36mrun_training_kfold\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m第 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 折: 初始化模型及相关组件...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# !! 每次循环都重新创建模型，保证每折独立训练 !!\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43march\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_ARC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENCODER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENCODER_WEIGHTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     81\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CombinedLoss(dice_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, focal_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m)\n\u001b[1;32m     82\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, weight_decay\u001b[38;5;241m=\u001b[39mWEIGHT_DECAY)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/__init__.py:69\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(arch, encoder_name, encoder_weights, in_channels, classes, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong architecture type `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`. Available options are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     66\u001b[0m             arch, \u001b[38;5;28mlist\u001b[39m(MODEL_ARCHITECTURES_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     67\u001b[0m         )\n\u001b[1;32m     68\u001b[0m     )\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/base/hub_mixin.py:147\u001b[0m, in \u001b[0;36msupports_config_loading.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    146\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/decoders/unet/model.py:76\u001b[0m, in \u001b[0;36mUnet.__init__\u001b[0;34m(self, encoder_name, encoder_depth, encoder_weights, decoder_use_batchnorm, decoder_channels, decoder_attention_type, in_channels, classes, activation, aux_params, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@supports_config_loading\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     73\u001b[0m ):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m UnetDecoder(\n\u001b[1;32m     85\u001b[0m         encoder_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[1;32m     86\u001b[0m         decoder_channels\u001b[38;5;241m=\u001b[39mdecoder_channels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m         attention_type\u001b[38;5;241m=\u001b[39mdecoder_attention_type,\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head \u001b[38;5;241m=\u001b[39m SegmentationHead(\n\u001b[1;32m     94\u001b[0m         in_channels\u001b[38;5;241m=\u001b[39mdecoder_channels[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     95\u001b[0m         out_channels\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[1;32m     96\u001b[0m         activation\u001b[38;5;241m=\u001b[39mactivation,\n\u001b[1;32m     97\u001b[0m         kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     98\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/encoders/__init__.py:86\u001b[0m, in \u001b[0;36mget_encoder\u001b[0;34m(name, in_channels, depth, weights, output_stride, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     82\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong pretrained weights `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` for encoder `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`. Available options are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     83\u001b[0m                 weights, name, \u001b[38;5;28mlist\u001b[39m(encoders[name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     84\u001b[0m             )\n\u001b[1;32m     85\u001b[0m         )\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_zoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m encoder\u001b[38;5;241m.\u001b[39mset_in_channels(in_channels, pretrained\u001b[38;5;241m=\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m32\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/encoders/efficientnet.py:81\u001b[0m, in \u001b[0;36mEfficientNetEncoder.load_state_dict\u001b[0;34m(self, state_dict, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m state_dict\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fc.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m state_dict\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fc.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2138\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2131\u001b[0m         out \u001b[38;5;241m=\u001b[39m hook(module, incompatible_keys)\n\u001b[1;32m   2132\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   2133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2135\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit should be done inplace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2136\u001b[0m         )\n\u001b[0;32m-> 2138\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2126\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2124\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2125\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)}\n\u001b[0;32m-> 2126\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2126\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2124\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2125\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)}\n\u001b[0;32m-> 2126\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2126\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2124\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2125\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)}\n\u001b[0;32m-> 2126\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2120\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[1;32m   2119\u001b[0m     local_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massign_to_params_buffers\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m assign\n\u001b[0;32m-> 2120\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2040\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2040\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2042\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhile copying the parameter named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2043\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the model are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2044\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the checkpoint are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_param\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2045\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124man exception occurred : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2046\u001b[0m                       )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === 11. 主执行入口 ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 执行 K-Fold 训练 ---\n",
    "    # run_training_kfold() 返回一个包含所有折数最佳模型路径的列表\n",
    "    list_of_best_models = run_training_kfold()\n",
    "\n",
    "    # --- 执行 K-Fold 集成预测 ---\n",
    "    if list_of_best_models and len(list_of_best_models) > 0:\n",
    "        print(f\"\\n使用 {len(list_of_best_models)} 个 K-Fold 模型进行集成预测...\")\n",
    "        predict_test_set_kfold(list_of_best_models)\n",
    "    else:\n",
    "        # 如果训练失败或未找到模型，尝试从输出目录加载已有的 fold 模型\n",
    "        print(\"\\n训练未生成模型或失败，尝试加载输出目录中已存在的 K-Fold 模型...\")\n",
    "        existing_fold_models = [os.path.join(OUTPUT_DIR, f) for f in os.listdir(OUTPUT_DIR) if f.startswith(\"best_model_fold_\") and f.endswith(\".pth\")]\n",
    "        if existing_fold_models:\n",
    "             print(f\"找到 {len(existing_fold_models)} 个已存在的 K-Fold 模型，将使用它们进行预测。\")\n",
    "             predict_test_set_kfold(existing_fold_models)\n",
    "        else:\n",
    "             print(\"\\n未找到任何 K-Fold 模型文件，无法进行预测。\")\n",
    "\n",
    "    print(\"\\n===== K-Fold 工作流执行完毕 =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5907e22-b9d5-4aec-bd5c-834009c0dbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始手动触发 K-Fold 集成预测流程...\n",
      "在目录 'output/' 中查找模型文件...\n",
      "  找到: best_model_fold_0.pth\n",
      "  找到: best_model_fold_1.pth\n",
      "  找到: best_model_fold_2.pth\n",
      "  找到: best_model_fold_3.pth\n",
      "  找到: best_model_fold_4.pth\n",
      "\n",
      "共找到 5 个模型文件，将使用它们进行集成预测。\n",
      "\n",
      "===== 开始对测试集进行 K-Fold 集成预测 (无表头输出, 使用内建 csv 模块) =====\n",
      "从 数据集/test_sample_submit.csv 加载测试集文件顺序...\n",
      "2500 个原始测试文件名加载成功。\n",
      "清理文件名中的空白字符...\n",
      "清理后的第一个文件名示例: 'R05K5826G4.jpg'\n",
      "使用清理后的路径进行示例检查...\n",
      "清理后的示例测试图片路径检查通过。\n",
      "创建测试集 Dataset 和 DataLoader...\n",
      "测试集 DataLoader 创建完成。\n",
      "加载 5 个 K-Fold 模型...\n",
      "  加载: output/best_model_fold_0.pth\n",
      "  加载: output/best_model_fold_1.pth\n",
      "  加载: output/best_model_fold_2.pth\n",
      "  加载: output/best_model_fold_3.pth\n",
      "  加载: output/best_model_fold_4.pth\n",
      "成功加载 5 个模型。\n",
      "开始集成预测...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f868248bea5d4f3aaa5b64e57a1a86be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "集成预测中:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "集成预测完成。共生成 2500 条结果。\n",
      "创建提交文件 (无表头, 使用内建 csv 模块)...\n",
      "正在使用 Python 内建 csv 模块保存提交文件到: output/submission_kfold.csv (无表头)...\n",
      "提交文件已成功保存到: output/submission_kfold.csv\n",
      "提交文件前 5 行预览 (注意可能有引号):\n",
      "\"R05K5826G4.jpg\",\"21 20 532 24 1044 27 1555 29 2067 30 2578 32 3089 35 3600 37 4111 39 4622 42 5133 44 5645 45 6156 47 6667 49 7179 49 7690 51 8201 53 8713 54 9224 56 9735 59 10247 60 10758 65 11269 82 11781 87 12292 91 12803 94 13315 98 13826 106 13938 3 14337 122 14849 126 15361 135 15874 142 16386 148 16897 157 17409 165 17921 170 18433 177 18945 182 19457 185 19969 191 20481 200 20993 202 21505 204 22017 219 22529 223 23041 230 23553 237 24065 245 24577 249 25089 250 25601 251 26113 261 26625 276 27137 284 27649 59 27712 226 28161 58 28226 230 28673 54 28750 221 29185 52 29266 221 29697 51 29802 201 30209 50 30315 203 30721 48 30827 206 31233 47 31339 208 31745 45 31852 210 32257 44 32364 216 32769 43 32877 218 33283 3 33301 3 33310 13 33391 219 33824 10 33906 219 34338 6 34426 4 34433 207 34947 208 35461 211 35975 27 36004 183 36491 22 36516 186 37008 16 37028 189 37526 9 37541 191 38053 195 38565 198 39077 201 39590 203 40103 204 40449 4 40616 207 40961 5 41130 210 41473 6 41644 210 41985 7 42161 208 42497 9 42676 208 43009 10 43191 209 43521 11 43705 211 44033 12 44220 16 44243 188 44545 13 44741 4 44764 182 45057 15 45286 175 45569 16 45801 175 46081 17 46325 166 46593 18 46839 40 46890 115 47105 20 47353 35 47405 114 47617 21 47874 24 47922 111 48129 22 48437 110 48641 24 48950 111 49153 25 49464 114 49665 26 49977 116 50177 27 50490 118 50689 29 51002 121 51201 30 51514 124 51713 32 52027 127 52225 34 52539 130 52737 37 53052 131 53249 45 53566 6 53579 117 53761 51 54095 114 54273 53 54612 110 54785 55 55127 107 55297 56 55643 104 55809 57 56158 102 56321 60 56673 99 56833 66 57189 96 57345 75 57424 7 57704 93 57857 93 57972 9 58221 89 58369 104 58482 13 58736 86 58881 127 59250 85 59393 128 59764 83 59905 131 60276 84 60417 134 60665 2 60789 84 60929 138 61173 9 61301 84 61441 143 61683 12 61814 14 61834 63 61953 147 62194 13 62327 11 62351 59 62465 150 62623 2 62705 15 62840 8 62866 56 62977 152 63132 12 63154 12 63216 16 63354 4 63381 53 63489 193 63728 16 63897 50 64001 194 64239 18 64411 48 64513 195 64750 19 64925 46 65025 197 65262 19 65438 46 65537 200 65773 21 65951 45 66049 205 66285 22 66463 46 66561 215 66796 23 66975 46 67073 221 67308 24 67487 46 67585 231 67818 27 67999 46 68097 261 68512 46 68609 264 69024 46 69121 268 69537 45 69633 274 70050 45 70145 276 70563 44 70658 276 71075 44 71171 276 71587 45 71683 4 71690 270 72100 44 72213 259 72612 44 72733 252 73124 45 73251 246 73636 45 73768 241 74148 45 74284 237 74660 46 74800 233 75172 46 75325 221 75572 7 75684 46 75840 219 76083 9 76196 46 76354 218 76594 11 76708 47 76869 218 77106 11 77221 46 77385 220 77617 13 77733 47 77903 220 78128 14 78245 47 78420 235 78758 47 78941 227 79270 47 79455 227 79783 47 79971 228 80241 8 80295 47 80491 226 80752 10 80806 49 81009 224 81264 11 81318 50 81527 227 81776 11 81829 51 82046 226 82287 13 82341 51 82563 230 82798 14 82852 53 83084 224 83309 15 83364 53 83614 223 83875 54 84137 213 84387 55 84654 211 84899 55 85171 212 85411 55 85693 207 85922 56 86215 203 86434 57 86733 199 86945 58 87248 198 87456 59 87764 264 88284 256 88800 253 89316 7 89326 239 89841 236 90357 233 90880 222 91394 219 91910 215 92428 209 92947 202 93463 197 93983 189 94500 183 95026 169 95541 165 96057 160 96575 154 97089 151 97604 147 98121 142 98640 134 99159 126 99676 120 100197 110 100714 105 101234 95 101754 87 102271 81 102788 74 103311 61 103833 50 104358 35 104873 31 105390 24 105909 14 137783 6 138292 16 138804 25 139315 32 139827 39 140339 46 140851 54 141363 59 141875 64 142386 71 142898 75 143410 77 143922 78 144434 79 144946 79 145457 80 145969 80 146481 80 146993 80 147505 80 148016 81 148528 81 149040 81 149552 81 150064 81 150576 81 151088 81 151599 82 152111 82 152623 82 153135 82 153647 82 154159 82 154671 81 154871 8 155183 81 155352 6 155382 17 155695 81 155863 10 155893 27 156207 80 156374 15 156405 37 156719 80 156886 19 156917 41 157231 80 157398 20 157428 48 157743 80 157910 21 157940 53 157996 12 158255 80 158422 22 158452 77 158768 79 158933 23 158964 85 159281 77 159445 23 159476 87 159794 76 159957 23 159988 89 160309 73 160469 23 160500 90 160824 69 160980 24 161012 93 161340 65 161492 24 161524 95 161859 58 162004 23 162036 96 162375 54 162516 23 162548 98 162895 46 163028 23 163059 103 163414 38 163540 23 163571 106 163686 3 163932 32 164051 24 164084 107 164196 6 164452 24 164563 24 164596 118 164972 15 165075 24 165108 118 165491 6 165587 23 165621 118 166098 24 166135 116 166610 24 166651 112 167050 5 167122 24 167167 108 167556 13 167634 23 167700 19 167724 63 168066 16 168146 23 168221 7 168241 57 168578 17 168657 24 168764 46 169090 17 169169 24 169285 37 169602 17 169681 23 169813 20 170115 16 170193 23 170329 15 170627 16 170705 23 171139 16 171217 23 171652 15 171729 23 172164 15 172241 22 172676 15 172753 22 173188 14 173265 22 173700 14 173777 22 174213 13 174290 20 174726 11 174803 19 175241 7 175318 1 175320 13 175835 10 176351 5\"\n",
      "\"NAWXJVLK1Y.jpg\",\"66 112 464 14 578 114 975 18 1089 116 1487 20 1601 116 1998 21 2112 117 2509 23 2624 117 3020 24 3136 117 3530 27 3649 116 4040 30 4135 2 4143 7 4161 116 4551 32 4644 18 4674 114 5061 36 5154 21 5186 114 5571 40 5664 24 5699 112 6082 43 6174 26 6211 111 6592 46 6684 29 6724 60 6787 46 7101 50 7194 31 7236 59 7300 44 7611 53 7704 34 7749 58 7812 42 8122 55 8214 36 8262 57 8325 37 8632 58 8724 39 8775 56 8838 34 9143 59 9235 40 9288 55 9351 31 9654 61 9745 43 9802 52 9864 25 10166 61 10257 43 10319 47 10378 17 10678 62 10768 45 10832 46 10893 11 11190 62 11280 46 11344 45 11406 7 11702 63 11792 46 11856 44 11919 5 12215 62 12304 47 12368 43 12727 63 12816 47 12880 43 13240 62 13328 48 13392 42 13683 5 13752 63 13841 47 13904 40 14192 17 14265 63 14353 48 14416 38 14701 23 14777 64 14866 48 14929 35 15211 26 15289 64 15378 48 15441 34 15721 29 15802 64 15891 47 15954 32 16232 31 16314 64 16403 48 16467 31 16743 33 16826 64 16915 48 16979 30 17253 35 17338 64 17428 48 17492 28 17764 37 17850 64 17940 48 18004 23 18276 38 18361 65 18453 47 18516 21 18787 39 18873 65 18965 48 19029 16 19298 41 19384 65 19477 48 19541 14 19809 42 19896 65 19990 48 20054 12 20321 43 20407 65 20502 48 20568 7 20832 45 20919 65 21015 47 21344 46 21430 65 21527 48 21856 46 21942 65 22040 48 22367 48 22455 63 22552 49 22879 49 22967 62 23065 48 23390 50 23479 61 23577 49 23902 51 23992 59 24089 49 24414 51 24504 57 24601 49 24926 52 25017 54 25114 49 25438 52 25502 5 25529 53 25625 50 25950 53 26011 9 26042 50 26137 50 26463 52 26521 12 26555 46 26649 51 26975 53 27031 15 27068 43 27161 51 27488 70 27581 40 27673 52 28000 71 28093 39 28185 52 28513 70 28606 38 28697 52 29026 70 29118 37 29210 51 29539 70 29631 35 29722 50 30051 71 30143 34 30235 49 30564 70 30655 33 30747 49 31076 71 31167 32 31259 48 31589 71 31679 30 31772 47 32101 71 32191 25 32284 29 32318 12 32614 70 32702 22 32796 26 32831 11 33127 70 33214 18 33309 24 33344 9 33640 69 33726 16 33821 22 33859 4 34153 68 34239 13 34307 2 34334 20 34665 68 34751 12 34818 4 34846 18 35178 67 35265 8 35329 6 35359 15 35690 66 35841 7 35871 13 36202 66 36353 8 36384 10 36715 65 36865 10 36897 7 37227 64 37377 11 37411 1 37740 63 37889 12 38252 62 38401 12 38765 60 38913 13 39277 60 39425 14 39790 58 39937 14 40303 57 40449 15 40815 57 40961 15 41328 56 41473 15 41840 57 41985 15 42353 56 42497 15 42865 56 43009 14 43378 55 43521 13 43795 1 43890 55 44035 10 44302 9 44403 53 44548 9 44806 20 44916 52 45061 6 45317 22 45428 52 45574 4 45828 24 45941 51 46338 26 46453 51 46847 30 46965 50 47356 34 47478 48 47864 40 47990 47 48376 41 48502 46 48888 43 49014 46 49399 45 49527 46 49912 45 50039 46 50424 47 50551 46 50936 48 51064 45 51449 48 51576 43 51961 49 52089 41 52474 48 52602 40 52987 47 53115 39 53499 47 53627 39 54012 46 54140 39 54524 45 54653 38 55037 43 55165 38 55523 6 55550 41 55678 38 56032 10 56063 38 56190 39 56542 13 56575 36 56703 39 57052 15 57088 33 57215 40 57562 18 57601 31 57728 39 58071 21 58113 29 58241 39 58581 24 58626 27 58753 40 59092 25 59138 27 59266 40 59603 27 59650 26 59778 40 60114 29 60163 25 60291 39 60625 31 60675 24 60803 39 61137 33 61187 24 61316 38 61648 35 61699 25 61828 38 62160 36 62212 24 62341 37 62671 38 62724 24 62853 37 63182 39 63236 25 63366 36 63694 40 63748 25 63878 36 64205 42 64260 25 64391 36 64716 43 64772 25 64903 36 65228 44 65284 25 65416 35 65740 44 65796 25 65928 35 66252 45 66309 24 66441 33 66764 45 66821 24 66953 32 67276 45 67334 23 67466 29 67788 45 67846 23 67978 26 68301 44 68358 22 68491 23 68814 43 68871 20 69003 21 69294 3 69326 44 69383 19 69516 18 69800 13 69839 43 69896 17 70028 16 70309 17 70352 42 70409 15 70541 14 70819 21 70865 41 70921 13 71054 12 71330 22 71377 42 71434 11 71567 8 71841 24 71890 41 71948 8 72080 5 72352 26 72402 41 72463 1 72863 28 72914 42 73373 31 73426 42 73875 41 73939 42 74384 45 74451 42 74895 47 74963 43 75407 47 75476 42 75918 49 75988 43 76430 50 76501 42 76942 51 77013 43 77454 51 77526 42 77966 52 78038 43 78478 53 78550 43 78990 53 79063 42 79503 53 79575 42 80015 53 80087 43 80527 52 80600 42 81040 51 81112 41 81355 4 81534 7 81552 51 81624 40 81867 6 82044 10 82065 50 82137 37 82379 6 82554 13 82578 48 82649 35 82891 6 83065 15 83091 47 83162 33 83404 3 83576 18 83604 45 83674 32 84087 20 84117 44 84186 31 84598 22 84630 42 84698 30 85109 23 85143 41 85211 29 85619 26 85656 39 85723 28 86130 27 86168 37 86235 20 86258 4 86641 28 86681 33 86748 16 86771 1 87152 30 87193 30 87261 13 87663 31 87706 26 87774 10 88174 32 88219 22 88289 5 88685 34 88732 19 89196 35 89244 18 89332 1 89707 37 89757 15 89843 2 90217 40 90270 14 90355 3 90728 42 90783 12 90867 3 91240 42 91297 9 91380 2 91752 43 91812 4 91880 4 92264 43 92392 4 92776 44 92904 6 93289 43 93417 6 93805 40 93930 6 94318 39 94442 7 94808 2 94830 40 94955 7 95318 5 95343 39 95467 8 95826 10 95855 39 95979 8 96335 14 96367 39 96491 9 96845 17 96879 40 97003 10 97355 20 97390 41 97515 10 97859 28 97902 41 98027 10 98368 32 98414 41 98539 10 98879 34 98926 41 99051 11 99390 35 99438 41 99563 11 99900 38 99951 41 100075 10 100411 39 100463 41 100588 9 100922 40 100976 41 101102 6 101433 42 101489 41 101616 1 101945 42 102001 41 102457 42 102513 42 102969 42 103026 42 103480 43 103538 43 103991 45 104050 44 104502 46 104563 44 105012 48 105075 44 105523 49 105588 44 106033 51 106100 44 106543 53 106612 45 107054 55 107125 44 107564 57 107637 44 108076 57 108149 45 108587 59 108661 45 109099 59 109173 45 109611 59 109685 45 109872 2 110124 59 110197 45 110377 12 110636 59 110709 45 110886 16 111149 58 111221 44 111396 19 111662 56 111734 43 111906 21 112175 55 112247 41 112416 23 112687 53 112759 41 112927 24 113200 51 113272 40 113437 26 113713 48 113785 39 113947 27 114226 45 114298 38 114457 29 114739 42 114811 37 114968 30 115252 38 115324 35 115478 32 115764 35 115837 34 115988 34 116276 34 116349 34 116499 34 116789 32 116862 11 116875 20 117010 35 117301 31 117376 6 117388 19 117521 36 117813 30 117890 2 117901 4 117911 8 118032 36 118325 29 118426 4 118543 37 118838 28 119054 38 119350 27 119566 38 119863 26 120078 37 120375 25 120590 37 120888 24 121102 37 121401 22 121614 37 121914 19 122126 37 122428 16 122638 37 122946 6 123150 37 123663 36 124176 35 124688 35 125201 34 125714 34 126227 35 126740 36 127253 37 127766 38 128280 37 128793 36 128846 6 129306 36 129356 9 129819 15 129837 18 129866 13 130333 12 130349 18 130377 15 130847 8 130861 19 130887 17 131088 6 131218 7 131361 5 131374 18 131398 19 131596 13 131727 13 131887 42 132104 20 132238 15 132400 41 132614 23 132748 18 132912 42 133124 26 133259 19 133424 42 133635 28 133770 20 133936 42 134146 30 134281 21 134447 43 134657 32 134790 24 134959 44 135169 33 135299 27 135471 44 135681 34 135809 30 135983 44 136193 34 136321 30 136495 45 136705 33 136832 32 137007 45 137217 33 137303 2 137345 31 137519 45 137729 33 137814 4 137858 30 138031 46 138241 33 138324 6 138375 25 138543 46 138753 33 138834 5 138840 2 138888 24 139055 47 139265 33 139346 4 139352 3 139400 24 139567 47 139777 33 139854 14 139912 23 140079 48 140289 33 140365 12 140378 3 140424 23 140591 49 140801 33 140877 17 140936 23 141103 49 141314 32 141388 19 141448 22 141615 49 141827 32 141900 19 141960 21 142127 49 142340 31 142413 18 142472 20 142639 50 142853 30 142926 19 142984 19 143151 50 143365 30 143438 19 143496 16 143664 49 143878 29 143951 18 144008 14 144176 49 144390 29 144463 19 144520 12 144688 49 144903 27 144976 18 145033 10 145201 48 145415 27 145489 17 145546 7 145713 48 145928 25 146002 16 146061 2 146225 48 146440 24 146516 13 146738 47 146952 23 147029 12 147250 47 147464 23 147542 11 147763 46 147977 18 148055 9 148275 46 148490 2 148497 6 148567 9 148788 45 149080 7 149301 44 149592 6 149814 43 150327 42 150839 42 151352 41 151865 39 152377 39 152890 37 153402 37 153915 36 154427 35 154940 34 155452 34 155964 33 156477 32 156989 32 157502 31 158014 30 158526 30 159039 29 159551 28 160064 27 160576 27 161088 26 161600 26 162113 24 162625 24 163137 23 163650 22 164162 21 164675 19 165189 16 165702 15 165914 10 165966 5 166216 12 166424 13 166475 11 166730 9 166935 15 166986 13 167445 18 167496 16 167955 20 168007 18 168452 9 168464 24 168518 20 168962 38 169029 21 169473 41 169541 22 169985 41 170052 23 170497 42 170564 23 171009 42 171077 22 171521 42 171588 22 172033 43 172100 22 172545 43 172612 21 173057 43 173124 20 173569 43 173637 18 174081 43 174150 16 174593 43 174664 12 175105 43 175179 3 175617 43 176129 43 176641 43 177153 43 177665 43 178177 44 178689 44 179201 43 179713 43 180225 41 180737 40 181249 38 181761 27 182273 25 182785 23 183297 20 183809 19 184321 17 184833 15 185345 13 185857 11 186369 10 186882 7 187396 3 202157 1 202665 7 203174 11 203685 14 204196 24 204708 25 205219 28 205731 30 206243 31 206754 32 207266 32 207777 33 208289 34 208800 35 209312 35 209620 10 209824 35 210129 15 210336 36 210637 20 210848 36 211144 25 211360 36 211653 29 211872 36 212163 31 212384 36 212674 32 212896 36 213186 33 213408 36 213696 35 213921 36 214206 37 214433 36 214715 40 214945 37 215223 44 215458 36 215732 47 215971 36 216242 49 216484 36 216752 51 216996 36 217261 54 217509 35 217771 57 218021 35 218281 59 218534 35 218790 63 219046 35 219301 65 219558 36 219811 69 220070 36 220323 70 220582 36 220835 71 221094 36 221347 71 221606 36 221858 73 222119 35 222370 73 222631 34 222882 74 223143 34 223393 75 223656 33 223904 76 224168 32 224415 78 224680 31 224926 79 225193 26 225436 82 225705 24 225947 83 226218 22 226458 84 226730 20 226970 85 227242 18 227481 86 227755 15 227993 86 228268 12 228505 87 228781 9 229017 87 229294 5 229530 87 230042 87 230261 1 230554 87 230764 2 230769 6 231066 88 231272 15 231578 88 231777 23 232091 87 232286 26 232603 87 232794 29 233116 86 233302 33 233628 86 233812 36 234141 85 234322 37 234653 85 234832 40 235165 86 235343 41 235678 84 235855 42 236190 84 236366 43 236702 84 236878 44 237215 82 237389 45 237727 82 237899 47 238239 80 238411 48 238752 66 238820 8 238923 48 239264 64 239436 47 239777 57 239835 1 239838 1 239948 47 240289 44 240340 5 240461 46 240802 41 240854 1 240973 46 241315 38 241487 44 241827 35 241999 44 242340 31 242510 45 242645 16 242852 29 243022 45 243153 21 243365 13 243379 12 243532 47 243663 24 243878 6 243895 6 244043 47 244174 25 244551 51 244684 27 245061 53 245195 29 245226 1 245571 55 245706 37 246082 56 246217 40 246593 56 246728 42 247105 56 247239 43 247617 55 247748 47 248129 54 248258 50 248641 54 248769 51 249154 53 249280 53 249666 53 249792 53 250178 53 250303 55 250690 52 250814 57 251202 52 251326 57 251715 51 251837 58 252227 50 252350 57 252739 50 252862 57 253252 49 253375 56 253764 49 253888 54 254277 49 254400 52 254789 16 254808 30 254913 47 255302 15 255321 30 255425 46 255815 13 255834 29 255938 44 256328 6 256346 29 256450 43 256859 28 256963 41 257372 27 257475 40 257884 27 257988 39 258397 27 258501 38 258909 27 259013 37 259422 26 259526 36 259934 26 260038 36 260446 26 260550 36 260959 25 261062 35 261473 23 261575 34 261986 21 262089 30\"\n",
      "\"FLS7SRM1YA.jpg\",\"337 38 505 6 847 41 1016 8 1357 44 1527 10 1867 47 2039 10 2378 47 2550 11 2889 48 3062 11 3400 49 3574 11 3910 51 4085 12 4420 53 4596 13 4931 54 5107 14 5442 54 5619 14 5953 55 6131 14 6464 56 6643 14 6976 57 7156 13 7487 58 7668 13 7999 58 8180 13 8511 58 8693 12 9023 57 9205 12 9536 55 9718 11 10048 54 10230 11 10560 53 10743 10 11073 51 11255 10 11585 50 11768 9 12098 48 12280 9 12611 46 12792 9 13123 44 13305 8 13635 42 13818 7 14148 40 14331 5 14661 37 14844 3 15173 36 15686 34 16200 31 16713 29 17227 26 17740 24 18253 20 18767 3 18772 10 19285 7 19799 3 19835 1 20344 6 20848 15 21350 5 21356 20 21859 29 22371 30 22883 31 23395 32 23907 34 24419 35 24931 36 25443 36 25956 35 26468 35 26979 37 27491 37 28002 39 28513 40 29023 43 29534 45 30045 47 30557 48 31070 48 31582 49 32094 50 32607 50 33120 49 33632 4 33639 43 34145 3 34152 43 34657 2 34665 42 35178 41 35691 40 36203 40 36716 38 37228 38 37740 37 38252 36 38764 36 39277 35 39789 35 40302 35 40815 34 41327 34 41840 32 42352 32 42865 30 43377 29 43890 28 44402 27 44915 25 45427 24 45940 22 46452 21 46964 20 47476 18 47521 9 47988 16 48029 14 48500 14 48539 17 48654 3 49013 12 49048 21 49163 9 49527 8 49557 24 49674 12 50041 4 50066 28 50184 16 50576 31 50693 20 51087 32 51203 23 51597 35 51713 26 52108 36 52225 26 52618 39 52737 27 53129 40 53249 28 53639 42 53761 28 54149 45 54273 29 54660 46 54785 29 55170 48 55297 29 55680 50 55809 30 56191 51 56321 30 56702 52 56833 30 57213 53 57345 31 57724 54 57857 31 58234 56 58369 32 58744 58 58881 32 59254 60 59393 33 59765 61 59905 33 60277 61 60417 33 60789 61 60929 34 61301 62 61441 34 61814 61 61953 34 62326 62 62465 34 62839 61 62977 35 63351 61 63489 35 63864 60 64001 34 64376 60 64513 35 64889 59 65025 35 65401 59 65538 35 65914 58 66051 36 66426 58 66563 37 66939 56 67076 37 67451 55 67588 38 67964 53 68100 23 68124 14 68476 50 68612 22 68636 15 68989 47 69125 20 69148 15 69502 45 69639 17 69659 16 70014 43 70153 14 70170 17 70527 37 70667 10 70680 19 71039 35 71191 20 71552 32 71702 21 72065 29 72214 21 72577 28 72725 22 73089 27 73237 21 73602 25 73749 20 74114 24 74261 19 74626 24 74774 17 75132 29 75287 15 75642 29 75800 9 76152 23 76313 6 76662 23 77171 25 77680 26 78179 38 78687 41 79198 42 79708 44 80219 46 80730 47 81241 49 81752 50 82264 50 82373 7 82775 52 82880 14 83287 52 83390 17 83799 52 83900 19 84311 51 84409 23 84824 50 84919 25 85336 50 85428 29 85849 49 85938 31 86361 48 86447 34 86874 45 86956 37 87386 44 87466 39 87899 41 87977 40 88070 1 88411 39 88488 41 88579 7 88924 37 89000 41 89091 8 89436 33 89511 42 89602 10 89949 27 90023 42 90113 11 90461 25 90535 43 90625 11 90974 23 91048 42 91137 12 91235 3 91486 22 91560 42 91649 13 91744 8 91999 19 92072 42 92161 13 92254 11 92512 15 92566 6 92584 42 92673 14 92764 13 93024 12 93074 12 93096 42 93185 15 93275 15 93537 9 93584 14 93608 42 93697 15 93787 16 94051 2 94095 16 94121 41 94209 16 94298 17 94606 17 94633 41 94721 17 94809 19 95117 18 95145 41 95233 18 95320 20 95629 19 95658 39 95745 19 95830 22 96140 20 96170 39 96257 19 96340 25 96652 20 96683 36 96769 20 96849 28 97165 20 97194 36 97281 21 97359 31 97677 20 97706 33 97793 21 97870 32 98189 20 98218 30 98305 22 98380 35 98701 20 98730 29 98817 22 98890 37 99214 20 99243 27 99329 23 99400 40 99726 20 99755 26 99841 23 99911 41 100238 21 100267 25 100353 24 100422 42 100751 21 100779 23 100865 24 100933 42 101264 14 101280 3 101291 18 101377 24 101444 43 101777 11 101803 15 101889 24 101955 43 102290 8 102320 6 102402 23 102466 43 102803 3 102914 23 102978 42 103427 21 103489 37 103940 19 104001 36 104454 16 104513 36 104970 9 105024 38 105537 37 106049 37 106561 38 107055 11 107074 37 107459 6 107565 15 107586 37 107969 10 108074 20 108099 36 108479 13 108583 23 108611 36 108990 15 109025 5 109093 26 109124 35 109500 18 109534 10 109604 27 109637 34 110011 20 110044 13 110115 29 110150 33 110521 22 110554 16 110626 30 110662 32 111031 25 111065 18 111137 31 111174 31 111541 28 111575 21 111644 36 111687 30 112051 32 112085 23 112152 40 112199 28 112560 61 112663 41 112710 27 113067 66 113173 44 113219 29 113574 71 113683 76 114084 73 114194 76 114594 76 114704 76 115104 78 115215 75 115614 81 115726 75 116125 82 116237 75 116635 84 116748 63 116813 10 117145 87 117259 63 117327 7 117655 89 117770 63 117841 2 118165 92 118281 64 118676 94 118792 64 119187 96 119304 64 119699 96 119815 64 120211 97 120327 64 120723 98 120838 64 121236 97 121350 63 121748 98 121862 62 122260 98 122374 62 122773 97 122886 61 123285 97 123398 61 123798 96 123911 59 124310 95 124423 58 124823 94 124935 57 125336 93 125448 55 125849 91 125960 54 126361 90 126472 51 126874 88 126985 49 127388 84 127498 47 127900 82 128010 46 128413 79 128523 44 128926 77 129035 43 129438 76 129548 41 129950 74 130061 39 130463 72 130573 38 130975 70 131086 36 131487 68 131599 33 131999 66 132111 31 132512 62 132624 28 133024 60 133136 26 133537 57 133649 23 134050 53 134162 21 134563 49 134675 18 135076 45 135188 15 135589 16 135608 23 135702 10 136102 12 136123 18 136220 2 136616 7 136636 14 137149 11 137663 7 138176 4 150449 5 150954 14 151465 16 151976 17 152487 18 152999 18 153511 17 154023 17 154536 15 155048 14 155561 11 156073 9 156587 5 158336 5 158846 8 159356 12 159867 14 160378 15 160890 15 161401 16 161912 18 162422 20 162933 21 163443 24 163954 25 164466 26 164977 27 165306 4 165488 29 165817 5 165998 31 166329 6 166399 1 166507 35 166840 7 166908 5 167016 38 167352 6 167419 6 167525 41 167864 6 167930 7 168035 43 168376 5 168441 8 168545 45 168888 4 168952 9 169055 47 169401 1 169464 9 169564 49 169976 9 170075 49 170488 9 170586 49 171001 8 171097 49 171513 8 171608 49 172026 7 172119 49 172538 7 172631 48 173051 6 173142 48 173563 6 173653 49 174075 6 174164 50 174588 5 174675 51 175101 4 175186 53 175613 4 175697 54 176126 3 176208 55 176719 57 177231 57 177743 57 178255 58 178767 58 179279 58 179792 57 180304 57 180817 56 181329 57 181842 57 182354 27 182384 28 182867 23 182897 28 183380 19 183409 29 183894 14 183921 29 184408 8 184434 29 184947 28 185460 28 185974 26 186487 24 187000 22 187513 20 188026 18 188539 15 189052 13 189565 11 190079 8 190592 6 191106 2 199831 6 200340 10 200834 29 201341 36 201851 40 201924 5 202362 42 202434 11 202874 43 202946 12 203385 44 203457 15 203897 45 203969 15 204408 47 204481 16 204920 47 204993 17 205432 47 205505 17 205944 47 206017 18 206456 47 206529 19 206967 49 207042 18 207479 49 207554 18 207991 49 208066 17 208502 51 208578 16 209014 51 209091 15 209526 52 209603 14 210038 52 210115 13 210550 52 210628 10 211062 52 211142 7 211575 51 211655 4 212088 50 212483 3 212601 49 212993 6 213114 48 213505 7 213627 46 214017 8 214141 43 214529 9 214654 41 215041 10 215168 38 215553 10 215681 35 216065 11 216194 33 216577 12 216707 30 217089 13 217219 28 217601 14 217732 26 218113 15 218245 23 218344 2 218625 16 218757 22 218851 9 219137 16 219270 19 219359 19 219649 17 219782 17 219870 26 220161 18 220295 14 220380 31 220673 18 220807 12 220890 35 221185 18 221320 10 221400 39 221697 18 221833 8 221911 49 222209 18 222346 5 222421 54 222721 18 222859 2 222932 57 223233 18 223443 58 223745 18 223954 60 224257 18 224466 59 224769 17 224977 59 225281 16 225489 59 225793 14 226001 59 226305 13 226514 56 226817 10 227026 55 227329 9 227539 53 227841 8 228051 51 228353 7 228563 50 228865 6 229076 47 229377 5 229588 47 229891 1 230101 45 230614 43 231126 43 231639 41 232152 40 232664 39 233177 36 233689 34 234201 33 234714 31 235227 28 235740 26 236253 23 236766 21 237279 18 237792 15 238266 6 238305 12 238776 8 238819 9 239286 11 239332 6 239796 13 240303 18 240813 21 241323 24 241352 6 241833 26 241863 8 242343 29 242375 9 242853 32 242887 9 243363 45 243874 46 244384 48 244895 49 245406 50 245917 51 246429 51 246941 51 247453 50 247965 50 248478 48 248990 47 249503 44 250016 42 250528 39 251041 36 251553 35 252066 33 252579 32 253092 31 253605 29 254118 28 254631 26 255143 23 255656 20 256168 18 256681 16 257194 14 257707 11\"\n",
      "\"XQLS9DXWMZ.jpg\",\"19371 8 19881 17 20392 23 20904 29 21415 35 21926 44 22437 50 22948 56 23459 61 23970 74 24482 79 24993 81 25505 82 26017 82 26528 84 27040 84 27552 85 28064 85 28576 85 29087 86 29599 87 30111 87 30623 87 31134 88 31646 87 32158 87 32669 88 33181 88 33693 88 33994 14 34205 88 34505 25 34716 89 35017 33 35228 89 35528 40 35740 88 36040 43 36252 88 36552 49 36764 88 37064 56 37125 5 37276 87 37575 72 37788 87 37946 11 38087 77 38299 88 38457 18 38599 81 38811 88 38969 27 39111 83 39323 88 39480 32 39622 88 39835 87 39992 35 40134 91 40347 87 40504 39 40646 95 40858 88 41016 41 41158 98 41370 88 41528 45 41669 103 41882 88 42039 51 42181 107 42394 87 42551 60 42693 111 42905 88 43063 63 43205 113 43417 88 43575 66 43717 114 43929 88 44087 68 44229 114 44441 88 44599 70 44740 116 44952 89 45110 73 45252 116 45464 89 45622 75 45764 116 45976 89 46134 75 46276 116 46488 89 46645 77 46788 115 47000 89 47156 78 47300 115 47511 89 47657 90 47812 115 48023 89 48168 92 48325 114 48535 89 48679 93 48839 112 49047 89 49191 93 49354 108 49559 89 49702 95 49868 106 50071 89 50214 95 50381 105 50582 90 50725 96 50893 104 51094 89 51237 96 51405 104 51606 89 51749 96 51916 104 52118 89 52260 98 52428 104 52630 88 52772 98 52940 104 53141 89 53284 97 53452 104 53653 89 53795 99 53964 103 54165 89 54307 99 54476 103 54677 88 54819 100 54988 102 55188 89 55331 102 55500 102 55700 89 55843 104 56011 103 56212 89 56354 106 56523 102 56724 89 56866 107 57035 102 57236 89 57378 108 57547 102 57747 90 57890 108 58058 103 58259 90 58402 108 58570 102 58771 90 58913 109 59082 102 59283 90 59425 109 59594 102 59795 89 59937 109 60106 101 60306 90 60449 109 60617 102 60818 90 60960 110 61129 102 61330 90 61472 110 61641 102 61842 90 61984 110 62153 101 62354 90 62495 111 62665 101 62866 90 63007 111 63177 101 63378 89 63519 110 63689 101 63890 89 64031 110 64200 102 64401 90 64542 111 64712 101 64913 90 65054 111 65224 101 65425 89 65566 111 65736 100 65937 89 66078 111 66247 101 66449 89 66590 110 66759 100 66961 88 67101 111 67271 100 67473 88 67613 111 67783 100 67985 88 68125 111 68295 100 68496 89 68637 111 68806 101 69008 88 69149 111 69318 101 69520 88 69661 111 69830 102 70032 87 70173 111 70342 102 70543 88 70685 110 70854 102 71055 88 71196 111 71366 102 71567 88 71708 111 71878 102 72078 88 72220 111 72390 102 72590 88 72732 110 72901 103 73102 88 73244 110 73413 103 73613 89 73755 110 73925 103 74125 88 74267 110 74437 103 74637 88 74779 110 74949 103 75148 89 75291 109 75461 103 75660 89 75802 110 75973 103 76171 89 76314 110 76485 103 76683 89 76826 110 76997 103 77194 90 77338 110 77509 103 77706 90 77850 109 78021 103 78218 90 78361 110 78533 103 78729 91 78873 110 79045 102 79241 91 79385 110 79556 103 79753 90 79897 110 80068 103 80265 90 80409 109 80580 103 80776 91 80921 109 81092 103 81288 90 81433 109 81604 103 81800 90 81944 110 82116 102 82312 90 82456 110 82627 103 82824 90 82968 110 83139 103 83335 91 83479 111 83651 103 83847 90 83991 110 84163 103 84359 90 84503 110 84674 104 84871 90 85015 110 85186 103 85383 89 85527 110 85697 104 85894 90 86039 110 86209 104 86406 90 86550 111 86721 103 86918 90 87062 111 87233 103 87430 89 87574 110 87744 104 87941 90 88086 110 88256 103 88453 90 88598 110 88768 103 88965 89 89110 109 89279 103 89476 90 89622 109 89791 103 89988 90 90134 109 90303 102 90500 89 90646 109 90814 102 91012 89 91158 109 91326 101 91524 89 91669 109 91838 100 92035 90 92181 109 92350 100 92547 90 92693 109 92861 100 93059 90 93205 109 93373 100 93571 90 93717 109 93885 100 94083 89 94229 109 94397 100 94594 90 94741 109 94908 102 95106 90 95253 109 95420 102 95617 91 95765 109 95932 102 96129 91 96276 109 96443 103 96641 91 96788 109 96955 103 97153 91 97300 109 97467 102 97664 92 97812 109 97979 102 98176 91 98324 109 98491 102 98688 91 98836 109 99002 103 99200 91 99347 110 99514 103 99711 92 99859 110 100026 103 100223 44 100280 35 100371 110 100538 103 100735 43 100794 33 100866 6 100882 111 101049 104 101247 42 101313 25 101377 128 101561 104 101758 43 101829 21 101889 128 102073 103 102270 43 102346 15 102401 128 102585 103 102782 44 102861 10 102913 128 103097 103 103294 45 103425 127 103608 104 103806 46 103937 127 104120 104 104317 47 104449 127 104632 103 104829 47 104961 127 105144 103 105341 47 105473 127 105656 103 105853 47 105985 127 106167 104 106364 48 106497 127 106679 103 106876 48 107009 127 107191 103 107388 47 107521 127 107702 104 107900 47 108033 128 108214 104 108411 48 108545 128 108726 104 108923 48 109057 129 109238 103 109435 48 109569 130 109749 104 109947 48 110081 132 110261 104 110459 48 110593 133 110773 104 110970 48 111105 136 111285 104 111482 48 111617 139 111796 105 111994 47 112130 141 112308 105 112506 47 112644 145 112790 5 112819 106 113018 46 113162 147 113331 106 113530 45 113679 143 113843 106 114043 44 114194 141 114355 105 114556 42 114710 137 114866 106 115070 40 115226 134 115378 106 115588 33 115708 2 115742 131 115890 106 116108 24 116219 4 116259 128 116402 106 116626 17 116731 4 116776 127 116913 107 117145 7 117242 4 117294 125 117424 108 117753 5 117810 233 118264 7 118327 228 118775 9 118844 222 119287 10 119361 193 119563 14 119799 10 119880 186 120082 6 120310 11 120398 181 120822 11 120914 177 121334 11 121434 169 121846 11 121951 164 122357 12 122466 161 122869 12 122983 155 123381 12 123499 151 123893 12 124018 144 124405 12 124534 140 124917 12 125051 135 125428 13 125568 129 125940 13 126085 124 126452 13 126601 120 126964 13 127117 116 127475 14 127633 112 127987 14 128150 107 128499 14 128668 100 129011 14 129185 95 129523 14 129702 90 130035 14 130218 86 130547 14 130731 84 131058 15 131244 83 131570 15 131756 83 132082 15 132268 82 132495 2 132593 16 132780 82 133004 10 133105 16 133292 81 133515 15 133617 16 133804 81 134026 20 134128 17 134315 81 134538 24 134640 17 134827 81 135049 37 135152 17 135339 81 135560 41 135664 17 135851 81 136072 44 136176 17 136363 80 136584 49 136688 17 136875 80 137095 55 137200 17 137386 80 137607 58 137712 17 137898 80 138119 60 138224 17 138409 80 138631 62 138736 17 138921 80 139143 63 139248 17 139432 81 139655 63 139760 17 139944 81 140166 64 140273 16 140456 81 140678 64 140784 17 140968 81 141190 64 141296 17 141480 82 141702 64 141807 18 141992 81 142214 64 142319 18 142504 82 142726 64 142831 18 143017 81 143237 65 143342 19 143529 81 143749 64 143854 19 144041 81 144261 64 144366 19 144552 82 144773 64 144878 19 145064 82 145284 65 145390 19 145577 81 145796 64 145902 19 146089 81 146307 65 146414 19 146600 83 146819 65 146926 19 147112 84 147330 65 147438 19 147624 85 147842 65 147950 19 148136 87 148354 65 148463 18 148648 89 148865 65 148975 18 149160 93 149378 64 149488 17 149672 97 149890 63 150001 16 150184 99 150402 63 150515 14 150696 100 150915 62 151029 11 151208 101 151429 60 151546 3 151720 101 151944 12 151959 42 152232 101 152462 1 152475 37 152744 101 152988 36 153256 101 153502 34 153769 100 154016 32 154281 100 154531 28 154795 98 155050 21 155307 98 155566 17 155819 97 156082 12 156331 97 156597 7 156843 97 157355 97 157867 97 158379 97 158891 97 159403 96 159915 96 160427 96 160938 96 161450 96 161962 96 162474 95 162986 94 163497 95 164009 95 164521 94 165033 94 165544 94 166056 93 166567 94 167079 93 167591 93 168102 94 168614 94 169126 94 169638 94 170150 94 170661 95 171173 95 171681 1 171685 95 172192 2 172197 95 172704 2 172709 95 173216 2 173221 95 173727 3 173733 95 174239 2 174244 96 174751 3 174756 96 175263 3 175267 97 175775 101 176286 102 176798 102 177310 101 177822 101 178333 102 178845 102 179356 103 179868 103 180380 103 180891 104 181403 104 181914 105 182426 104 182938 104 183449 105 183961 105 184473 105 184984 105 185496 105 186008 105 186520 105 187032 105 187544 105 188055 105 188567 105 189079 105 189591 105 190102 106 190614 105 191126 105 191638 105 192150 104 192662 104 193173 105 193685 105 194197 105 194709 105 195221 104 195733 104 196244 105 196756 105 197268 104 197780 104 198291 105 198803 105 199315 105 199827 105 200339 105 200850 106 201362 106 201874 105 202386 105 202897 106 203409 106 203921 106 204433 106 204945 105 205457 105 205969 105 206480 106 206992 105 207504 105 208016 104 208528 104 209039 105 209551 104 210063 104 210575 103 211086 104 211598 103 212110 103 212622 102 213134 101 213645 101 214157 100 214669 100 215181 100 215693 100 216205 100 216717 99 217228 100 217740 100 218252 100 218763 101 219139 2 219275 102 219649 7 219787 102 220161 17 220299 102 220673 22 220810 103 221185 26 221322 102 221697 31 221834 102 222209 34 222346 102 222721 38 222857 103 223233 42 223369 103 223745 47 223881 103 224257 52 224393 103 224769 55 224905 103 225281 59 225417 103 225793 65 225929 103 226305 70 226440 103 226817 74 226952 103 227329 76 227464 103 227841 77 227976 104 228353 78 228488 103 228865 78 228999 104 229377 79 229511 104 229889 79 230023 104 230401 80 230535 104 230913 80 231047 104 231425 80 231559 104 231937 80 232070 105 232449 80 232582 105 232961 79 233094 105 233473 79 233606 104 233985 79 234118 104 234497 79 234629 105 234819 3 235009 79 235141 105 235330 8 235521 79 235653 105 235841 15 236033 79 236165 105 236352 18 236545 79 236677 105 236864 20 237057 79 237188 106 237376 30 237569 79 237700 106 237888 35 238081 79 238212 106 238399 39 238441 5 238593 78 238723 106 238911 50 239105 78 239235 106 239423 52 239617 78 239747 106 239935 56 240129 78 240258 107 240447 60 240641 78 240770 106 240959 64 241153 78 241282 106 241471 66 241665 77 241793 107 241982 70 242177 77 242305 107 242494 73 242689 77 242817 107 243006 75 243201 77 243329 106 243518 77 243713 77 243841 106 244030 87 244225 76 244353 106 244542 90 244737 76 244864 107 245053 93 245249 76 245376 107 245565 94 245761 76 245888 107 246077 94 246273 76 246400 106 246589 94 246785 75 246912 106 247100 95 247297 75 247424 106 247612 95 247809 75 247936 106 248124 95 248321 75 248447 106 248635 96 248833 74 248959 106 249147 96 249345 74 249471 106 249659 95 249857 74 249983 106 250171 95 250369 74 250494 107 250683 94 250881 74 251006 106 251195 93 251393 73 251518 106 251707 93 251905 73 252029 107 252220 90 252417 73 252541 107 252732 88 252929 72 253053 107 253246 85 253441 72 253564 108 253760 83 253953 72 254076 107 254277 77 254465 72 254588 107 254792 74 254977 72 255100 107 255306 72 255489 71 255611 108 255820 54 255877 12 256001 71 256123 108 256334 51 256390 10 256513 71 256635 107 256848 49 256903 8 257025 71 257147 107 257362 47 257418 3 257537 71 257659 106 257882 39 258049 70 258171 89 258269 7 258402 30 258551 9 258561 70 258683 88 258917 27 259061 11 259073 71 259194 89 259431 11 259445 10 259572 12 259585 71 259706 89 259945 8 259958 8 260083 13 260097 71 260218 89 260459 5 260471 5 260595 13 260609 71 260730 89 261107 11 261121 70 261242 88 261633 69 261755 84\"\n",
      "\"J0P3C5OIB0.jpg\",\"18628 5 19127 21 19635 25 20146 26 20658 26 21169 26 21681 26 22194 25 22706 24 23217 24 23730 21 24243 18 24756 12 25269 6 66556 5 67065 8 67574 11 68084 13 68594 15 69019 6 69105 16 69527 11 69615 18 70037 13 70125 20 70545 18 70635 22 71053 22 71145 24 71559 28 71655 26 72067 33 72165 28 72576 36 72675 30 73084 40 73185 32 73591 45 73694 35 74100 48 74205 36 74609 52 74715 38 75119 54 75227 38 75629 56 75739 38 76139 58 76250 39 76650 60 76762 39 77162 60 77275 38 77674 60 77787 38 78186 60 78299 38 78698 60 78811 38 79210 60 79324 37 79722 61 79836 37 80234 61 80349 36 80747 60 80861 36 81259 60 81374 35 81771 61 81886 35 82284 60 82399 34 82796 61 82911 34 83309 61 83424 33 83821 61 83936 33 84333 62 84448 33 84846 61 84961 32 85358 62 85473 32 85870 62 85986 31 86277 4 86382 62 86498 31 86788 9 86895 62 87011 30 87299 15 87407 62 87524 29 87811 18 87919 63 88036 29 88323 37 88370 5 88431 63 88549 28 88835 54 88943 63 89061 28 89347 55 89456 63 89574 27 89859 56 89968 64 90086 27 90371 57 90480 67 90598 27 90883 57 90992 69 91111 26 91395 57 91504 69 91623 26 91908 56 92017 69 92136 25 92420 56 92529 69 92648 25 92932 56 93041 70 93160 25 93444 56 93554 69 93673 24 93956 56 94066 69 94185 24 94467 57 94578 69 94698 23 94979 57 95090 70 95210 23 95491 57 95602 70 95723 22 96003 57 96114 70 96235 22 96515 57 96626 70 96748 21 97027 56 97138 71 97260 21 97539 56 97650 71 97772 21 98051 56 98161 72 98285 20 98563 56 98672 73 98798 19 99075 56 99184 74 99310 19 99586 57 99695 75 99823 18 100098 57 100207 75 100336 17 100610 56 100719 75 100848 17 101122 56 101231 76 101361 16 101634 56 101743 76 101873 16 102146 56 102255 76 102386 15 102658 56 102767 76 102898 15 103170 56 103280 75 103410 15 103682 56 103792 75 103923 14 104194 55 104304 76 104435 14 104706 55 104816 76 104948 13 105218 55 105328 76 105460 13 105730 55 105840 76 105973 12 106242 55 106353 76 106485 12 106754 55 106865 76 106998 11 107266 55 107377 76 107510 11 107778 54 107889 76 108022 11 108290 54 108401 77 108535 10 108802 54 108914 76 109047 10 109314 54 109426 76 109559 10 109826 53 109938 77 110072 9 110338 53 110451 76 110584 9 110850 53 110963 76 111096 9 111362 53 111475 77 111608 9 111874 52 111987 77 112121 8 112386 52 112499 77 112633 8 112898 52 113012 77 113145 8 113410 52 113524 77 113658 7 113922 52 114036 78 114170 7 114434 52 114548 78 114682 7 114848 18 114945 53 115061 77 115195 6 115359 25 115457 53 115573 77 115707 6 115870 39 115969 53 116085 78 116220 5 116381 56 116481 53 116598 77 116733 3 116893 69 116993 53 117110 77 117405 71 117505 53 117622 77 117917 72 118017 54 118135 77 118429 72 118529 54 118647 77 118941 73 119041 54 119159 77 119453 73 119553 54 119671 77 119965 73 120065 55 120184 77 120476 74 120576 56 120696 77 120988 74 121088 56 121208 77 121500 74 121600 56 121720 78 122012 74 122112 56 122233 77 122524 74 122624 56 122745 77 123036 74 123136 56 123257 77 123548 74 123648 55 123769 77 124060 73 124160 55 124282 77 124572 73 124672 55 124794 77 125084 73 125184 55 125306 77 125595 74 125696 55 125818 77 126107 74 126208 55 126331 77 126619 74 126720 54 126843 77 127131 74 127232 54 127355 77 127643 74 127744 54 127868 76 128155 74 128256 53 128380 77 128667 75 128768 53 128892 77 129179 75 129279 54 129405 76 129691 75 129791 54 129917 77 130203 75 130303 54 130429 77 130715 75 130815 54 130941 77 131227 74 131327 54 131454 77 131739 74 131839 53 131966 77 132250 75 132351 53 132478 77 132762 75 132863 53 132991 76 133274 75 133375 53 133503 77 133786 75 133887 53 134015 77 134298 75 134399 53 134528 76 134810 75 134911 53 135040 77 135322 75 135423 53 135552 77 135834 75 135935 52 136064 77 136346 74 136447 52 136576 77 136859 73 136959 52 137089 76 137371 73 137471 52 137601 76 137884 72 137983 52 138113 76 138397 71 138495 52 138625 76 138910 11 138926 54 139007 52 139138 75 139423 3 139445 46 139519 52 139650 75 139958 45 140031 52 140162 74 140472 42 140543 52 140675 73 140985 40 141055 52 141187 71 141385 7 141509 27 141567 52 141699 70 141894 11 142026 1 142034 9 142079 52 142211 67 142404 14 142591 52 142724 64 142914 17 143103 52 143236 62 143424 20 143615 52 143748 59 143933 24 144126 53 144260 56 144443 26 144638 54 144772 55 144952 30 145150 53 145285 52 145463 31 145662 53 145797 49 145973 34 146174 53 146309 47 146483 36 146686 53 146822 45 146993 38 147198 53 147334 43 147503 41 147710 53 147846 42 148013 43 148221 54 148359 39 148523 45 148733 54 148871 38 149033 48 149245 54 149383 35 149544 49 149757 53 149895 34 150054 52 150269 53 150408 31 150565 53 150781 53 150920 30 151075 55 151293 53 151432 28 151586 56 151805 52 151944 26 152095 60 152317 52 152457 23 152606 61 152828 53 152969 21 153115 64 153340 53 153481 19 153603 1 153625 67 153852 53 153994 15 154114 3 154135 70 154364 53 154506 12 154625 5 154645 72 154876 53 155018 9 155137 6 155155 75 155388 53 155531 5 155649 6 155665 78 155900 53 156161 6 156175 81 156412 53 156673 8 156685 83 156924 53 157185 96 157436 52 157697 96 157948 53 158209 97 158460 53 158721 97 158972 53 159233 98 159483 54 159745 98 159995 54 160257 99 160507 54 160769 99 161019 54 161281 100 161531 54 161793 100 162043 54 162305 100 162555 54 162817 101 163067 54 163329 101 163580 53 163841 101 164092 52 164353 101 164604 52 164865 101 165117 50 165377 102 165631 47 165889 102 166145 44 166401 103 166669 31 166913 103 167199 8 167425 104 167937 104 168449 104 168961 105 169473 105 169985 106 170497 106 171010 106 171523 106 172035 106 172547 107 173059 107 173570 109 174081 111 174593 111 175105 111 175617 112 176129 112 176641 112 177153 113 177665 113 178177 114 178689 114 179201 115 179713 115 180225 116 180737 116 181249 117 181761 118 182273 119 182785 120 183297 120 183809 121 184321 122 184833 122 185345 123 185857 123 186369 124 186881 125 187393 125 187905 125 188417 126 188929 126 189441 127 189953 127 190465 128 190977 128 191489 128 192001 129 192513 129 193025 129 193537 130 194049 130 194561 131 195073 131 195585 131 196097 132 196609 133 197121 133 197633 134 198145 135 198657 135 199169 136 199681 137 200193 144 200705 145 201217 145 201729 146 202241 146 202753 147 203265 147 203777 148 204289 149 204801 150 205313 150 205825 150 206337 149 206849 148 207361 147 207873 145 208385 144 208897 144 209409 144 209921 143 210433 143 210945 143 211457 142 211969 142 212481 141 212993 141 213505 140 214017 139 214529 138 215041 137 215553 135 216065 133 216577 130 217089 128 217601 126 218113 124 218625 122 219137 120 219649 118 220161 116 220673 114 221185 112 221697 110 222209 108 222721 106 223233 104 223745 102 224257 101 224769 99 225281 98 225793 96 226305 94 226817 93 227329 91 227841 89 228353 87 228865 85 229377 83 229889 81 230401 80 230913 78 231425 76 231937 74 232449 72 232961 70 233473 68 233985 66 234497 65 235009 63 235521 61 236033 60 236545 57 237057 54 237569 53 238081 51 238593 50 239105 49 239617 48 240129 47 240641 47 241153 47 241665 46 242177 45 242689 43 243201 41 243713 39 244225 36 244737 34 245249 33 245761 32 246273 31 246785 30 247297 28 247809 27 248321 25 248833 23 249345 21 249857 19 250370 15 250883 3\"\n",
      "\n",
      "===== 手动预测流程执行完毕 =====\n"
     ]
    }
   ],
   "source": [
    "# === 手动触发 K-Fold 集成预测 ===\n",
    "\n",
    "print(\"开始手动触发 K-Fold 集成预测流程...\")\n",
    "\n",
    "# 1. 定义输出目录和模型文件前缀 (确保与训练时一致)\n",
    "output_dir = OUTPUT_DIR # 使用之前定义的全局变量\n",
    "model_prefix = \"best_model_fold_\"\n",
    "model_suffix = \".pth\"\n",
    "num_folds = N_FOLDS # 使用之前定义的全局变量\n",
    "\n",
    "# 2. 查找所有已保存的最佳模型文件路径\n",
    "fold_model_paths = []\n",
    "print(f\"在目录 '{output_dir}' 中查找模型文件...\")\n",
    "try:\n",
    "    for i in range(num_folds):\n",
    "        model_filename = f\"{model_prefix}{i}{model_suffix}\"\n",
    "        model_path = os.path.join(output_dir, model_filename)\n",
    "        if os.path.exists(model_path):\n",
    "            fold_model_paths.append(model_path)\n",
    "            print(f\"  找到: {model_filename}\")\n",
    "        else:\n",
    "            print(f\"  警告: 未找到模型文件 {model_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"查找模型文件时出错: {e}\")\n",
    "\n",
    "# 3. 检查是否找到了模型文件\n",
    "if not fold_model_paths:\n",
    "    print(\"\\n错误：在输出目录中未能找到任何 K-Fold 模型文件。无法进行预测。\")\n",
    "    print(f\"请确认 '{output_dir}' 目录中存在形如 '{model_prefix}*{model_suffix}' 的文件。\")\n",
    "else:\n",
    "    print(f\"\\n共找到 {len(fold_model_paths)} 个模型文件，将使用它们进行集成预测。\")\n",
    "    # 4. 调用预测函数\n",
    "    # 确保 predict_test_set_kfold 函数以及所有依赖项 (如 Dataset 类, 配置参数等)\n",
    "    # 在当前的 Jupyter Session 中已经被定义 (即之前的单元格已运行)\n",
    "    try:\n",
    "        predict_test_set_kfold(fold_model_paths)\n",
    "        print(\"\\n===== 手动预测流程执行完毕 =====\")\n",
    "    except NameError as ne:\n",
    "        print(f\"\\n错误：调用预测函数失败，可能之前的单元格未运行。错误信息: {ne}\")\n",
    "        print(\"请确保定义 predict_test_set_kfold 及相关配置/类的单元格已成功运行。\")\n",
    "    except Exception as ex:\n",
    "        print(f\"\\n执行预测时发生意外错误: {ex}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce5c74-1055-4b4a-89b0-e5521a0374ba",
   "metadata": {},
   "source": [
    "**如何运行:**\n",
    "\n",
    "1.  **确认所有路径** (在第 2 个代码单元格中) 正确指向您的数据目录和文件。\n",
    "2.  **检查超参数** (轮数、批大小、图片尺寸、学习率等) 并根据您的硬件和需求进行调整。\n",
    "3.  **选择模型架构和编码器** (`MODEL_ARC`, `ENCODER`)。`Unet` 配合 `efficientnet-b3` 或 `resnet34` 通常是不错的起点。\n",
    "4.  **运行所有单元格** (例如，通过菜单栏 \"Run\" -> \"Run All Cells\")。\n",
    "5.  训练过程将开始，最佳模型 (基于验证集 Dice 分数) 会保存在 `OUTPUT_DIR` 目录下 (默认为 `best_model.pth`)。同时会生成训练日志 (`training_log.csv`) 和训练曲线图 (`training_history.png`)。\n",
    "6.  训练结束后，程序会自动加载最佳模型，并对测试集进行预测。\n",
    "7.  最终的提交文件 (`submission.csv`) 会保存在 `OUTPUT_DIR` 目录下。\n",
    "\n",
    "**后续可能的改进方向:**\n",
    "\n",
    "*   **超参数调优:** 尝试不同的学习率、优化器参数、批大小、图片尺寸、损失函数权重等。\n",
    "*   **模型选择:** 测试 `segmentation-models-pytorch` 库中提供的其他模型架构和编码器。\n",
    "*   **交叉验证 (Cross-Validation):** 实现 K 折交叉验证，可以更可靠地评估模型性能，并通过集成 K 个模型的预测结果来提升最终性能。\n",
    "*   **测试时数据增强 (Test-Time Augmentation, TTA):** 在预测阶段对测试图片应用数据增强 (如翻转)，然后将多次预测的结果进行平均，通常能提高精度。\n",
    "*   **更复杂的数据增强:** 谨慎地尝试更多类型的数据增强方法。\n",
    "*   **后处理:** 对预测出的二值掩码进行处理，例如移除面积过小的连通区域，或者填充内部的小孔洞。\n",
    "\n",
    "**如何运行 (K-Fold 版本):**\n",
    "\n",
    "1.  **安装 Scikit-learn:** 如果您还没有安装，请运行 `pip install scikit-learn`。\n",
    "2.  **确认所有路径** (单元格 3) 正确。\n",
    "3.  **检查超参数** (单元格 3)，特别是 `N_FOLDS` (折数) 和 `EPOCHS` (每折轮数)。总训练时间约等于 `N_FOLDS * EPOCHS * 每轮时间`。\n",
    "4.  **选择模型架构和编码器** (单元格 3)。\n",
    "5.  **运行所有单元格**。\n",
    "6.  程序将依次训练 `N_FOLDS` 个模型，每个模型对应交叉验证的一折。每个模型训练时会进行 Early Stopping，并将最佳权重保存在 `output/best_model_fold_{折数}.pth`。\n",
    "7.  所有折训练完毕后，程序会自动加载这 `N_FOLDS` 个模型。\n",
    "8.  对测试集的每张图片，程序会用所有加载的模型进行预测，然后将预测的概率图进行平均。\n",
    "9.  基于平均后的概率图生成最终的二值掩码和 RLE 编码。\n",
    "10. 生成的集成预测结果将保存在 `output/submission_kfold.csv` 文件中。\n",
    "\n",
    "**K-Fold 的优势:**\n",
    "\n",
    "*   **更可靠的性能评估:** 验证集分数是基于所有数据的轮换验证得到的，更能反映模型在未知数据上的泛化能力。\n",
    "*   **更充分的数据利用:** 所有数据都被用于训练和验证。\n",
    "*   **模型集成:** 通过平均 K 个模型的预测，通常可以获得比单个模型更稳定、更精确的结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
